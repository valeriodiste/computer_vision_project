{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Vision Transformer Memory as a Differentiable Search Index for Image Retrieval](#toc0_)\n",
    "\n",
    "---\n",
    "\n",
    "## <a id='toc1_1_'></a>[üìö Notebook Overview](#toc0_)\n",
    "\n",
    "This notebook explores a novel [information retrieval (IR)](https://en.wikipedia.org/wiki/Information_retrieval) framework applied to image retrieval that utilizes a **differentiable function** to generate a **sorted list of image identifiers** in response to a given **image query**.\n",
    "\n",
    "The approach is called **Differentiable Search Index (DSI)**, and was originally proposed in the paper [Transformer Memory as a Differentiable Search Index](https://arxiv.org/pdf/2202.06991.pdf) by researchers at Google Research.\n",
    "\n",
    "In its original formulation, **DSI** aims at both encompassing all document's corpus information and executing retrieval within a single **Transformer language model**, instead of adopting the index-then-retrieve pipeline used in most modern IR sytems.\n",
    "\n",
    "The notebook presents the implemented DSI solution applied to an image retrieval task: a **Sequence to Sequence Vision Transformer** (ViT) model `f` that, given an image query `q` as input, returns a list of image IDs ranked by relevance to the given image query, and compares its performance with a traditional \"index-then-retrieve\" approach based on a **BoVW** baseline model.\n",
    "\n",
    "We evaluate the performance of the proposed models using the **Indexing Accuracy**, **Mean Average Precision (MAP)** and **Recall at K** metrics computed on multiple variations of the **ImageNet** and the **MS COCO** datasets, and we compare the results obtained for multiple ViT variations and  configurations with the aforementioned **BoVW** baseline.\n",
    "\n",
    "## <a id='toc1_2_'></a>[üìù Author](#toc0_)\n",
    "\n",
    "**Valerio Di Stefano** - _\"Sapienza\" University of Rome_\n",
    "<br/>\n",
    "Email: [distefano.1898728@studenti.uniroma1.it](mailto:distefano.1898728@studenti.uniroma1.it)\n",
    "\n",
    "## <a id='toc1_3_'></a>[üîó External Links](#toc0_)\n",
    "\n",
    "* **Main Related Work**: [Transformer Memory as a Differentiable Search Index](https://arxiv.org/pdf/2202.06991.pdf)\n",
    "\n",
    "  _Authors_: Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, Donald Metzler\n",
    "  \n",
    "* **Project Repository**: [GitHub Repository](https://github.com/valeriodiste/computer_vision_project)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[üìå Table of Contents](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [üìÑ Vision Transformer Memory as a Differentiable Search Index for Image Retrieval](#toc1_)    \n",
    "  - [üìö Notebook Overview](#toc1_1_)    \n",
    "  - [üìù Author](#toc1_2_)    \n",
    "  - [üîó External Links](#toc1_3_)    \n",
    "  - [üìå Table of Contents](#toc1_4_)    \n",
    "- [üöÄ Getting Started](#toc2_)    \n",
    "  - [Collect Source Files](#toc2_1_)    \n",
    "  - [Define Dataset](#toc2_2_)    \n",
    "  - [Install & Import Libraries](#toc2_3_)    \n",
    "  - [Configuration, Hyperparameters and Constants](#toc2_4_)    \n",
    "- [üíæ Data Preparation](#toc3_)    \n",
    "  - [Download Data & Resources](#toc3_1_)    \n",
    "  - [Database Creation](#toc3_2_)    \n",
    "  - [Indexing & Retrieval Datasets](#toc3_3_)    \n",
    "- [üõçÔ∏è Bag of Visual Words Model](#toc4_)    \n",
    "    - [BoVW Model Initialization](#toc4_1_1_)    \n",
    "- [ü§ñ Vision Transformer Model (DSI approach)](#toc5_)    \n",
    "    - [ViT Model Initialization](#toc5_1_1_)    \n",
    "- [üìà Evaluation](#toc6_)    \n",
    "  - [BoVW Model Evaluation](#toc6_1_)    \n",
    "  - [ViT Model Evaluation](#toc6_2_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <a id='toc2_'></a>[üöÄ Getting Started](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we check if we are running the notebook on Google colab or locally, defining the `RUNNING_ON_COLAB` constant used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on colab or locally\n",
    "try:\n",
    "\tfrom google.colab import files\n",
    "\tRUNNING_IN_COLAB = True\n",
    "\tprint(\"Running on Google Colab.\")\n",
    "except ModuleNotFoundError:\n",
    "\tRUNNING_IN_COLAB = False\n",
    "\tprint(\"Running locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc2_1_'></a>[Collect Source Files](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_1_'></a>[Clone Project's GitHub Repository](#toc0_)\n",
    "\n",
    "We **clone the project's repository** from GitHub to access the source files for datasets, models, evaluation and utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the git repository of the project for the source files\n",
    "!git clone https://github.com/valeriodiste/computer_vision_project.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_2_'></a>[Pull Latest Files Changes](#toc0_)\n",
    "\n",
    "We also **pull the latest changes** from the repository and store them in the `./computer_vision_project` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to the cloned repository\n",
    "%cd /content/computer_vision_project\n",
    "# Pull the latest changes from the repository\n",
    "!git pull origin main\n",
    "# Change the working directory to the parent directory\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Define Dataset](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **define the main dataset types** avilable:\n",
    "\n",
    "- **`COCO`**: Use the **MS COCO** dataset for image segmentation and object detection tasks.\n",
    "- **`IMAGENET_FULL`**: Use the full **ImageNet** dataset for image classification tasks.\n",
    "- **`IMAGENET_REDUCED`**: Use a small subset of the **ImageNet** dataset for image classification tasks, with less images and fixed square aspect ratio images from 160x160 to 512x512 pixels.\n",
    "- **`IMAGENET_TINY`**: Use the full **ImageNet** dataset with images cropped and resized to 64x64 pixels for image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the possible datasets\n",
    "class DatasetType:\n",
    "\tCOCO = \"coco\"\t\t\t\t\t\t\t# COCO dataset (using the COCO API for the image segmentation dataset)\n",
    "\tIMAGENET_FULL = \"imagenet_full\"\t\t\t# Full ImageNet dataset (heavy dataset, requires a HuggingFace token, not recommended)\n",
    "\tIMAGENET_REDUCED = \"imagenet_reduced\"\t# Reduced ImageNet dataset (less images, fixed square image sizes from 160x160 to 512x512, no token required)\n",
    "\tIMAGENET_TINY = \"imagenet_tiny\"\t\t\t# Tiny ImageNet dataset (contains all ImageNet images as 64x64 images, no token required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **define the dataset to use** for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset to use\n",
    "DATASET = DatasetType.IMAGENET_REDUCED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_3_'></a>[Install & Import Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_3_1_1_'></a>[Install Libraries](#toc0_)\n",
    "\n",
    "We **install all the necessary libraries** for this notebook.\n",
    "\n",
    "- **`pytorch-lightning`**: A **lightweight PyTorch wrapper** for simplifying PyTorch code.\n",
    "- **`wandb`**: The python package for **Weights & Biases**, a tool for experiment tracking, dataset versioning, and project collaboration (used for **logging and visualization**).\n",
    "- **`pycocotools`**: A Python API for the **MS COCO dataset**.\n",
    "- **`datasets`**: A library for easily **loading and preprocessing datasets** from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%%capture\n",
    "%pip install pytorch-lightning\n",
    "%pip install wandb\n",
    "\n",
    "# Install the packages required for the dataset\n",
    "if DATASET == DatasetType.COCO:\n",
    "\t%pip install pycocotools\n",
    "else:\n",
    "\t%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_3_1_2_'></a>[Import Modules](#toc0_)\n",
    "\n",
    "We then **import the required modules**, including `PyTorch`, `PyTorch Lightning`, `pycocotools`/`datasets` and `W&B`, plus other useful modules and libraries (`Numpy`, `CV2`, etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the standard libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# Import the PyTorch and Lighning libraries and modules\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Import the COCO or HuggingFace datasets module\n",
    "if DATASET == DatasetType.COCO:\n",
    "\tfrom pycocotools.coco import COCO\n",
    "else:\n",
    "\timport datasets as hf_datasets\n",
    "\n",
    "# Import the W&B (Weights & Biases) library\n",
    "import wandb\n",
    "from wandb.sdk import wandb_run\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Import other libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import base64\n",
    "\n",
    "# Import the tqdm library (for the progress bars) \n",
    "if not RUNNING_IN_COLAB:\n",
    "\tfrom tqdm import tqdm\n",
    "else:\n",
    "\tfrom tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also import our own **custom modules** (cloned from the repository) containing Python classes for **datasets**, **models**, **evaluation**, and **utilities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom modules\n",
    "if not RUNNING_IN_COLAB:\n",
    "\t# We are running locally (not on Google Colab, import modules from the \"src\" directory in the current directory)\n",
    "\tfrom src.scripts import models, datasets, training, evaluation, utils\t# type: ignore\n",
    "\tfrom src.scripts.utils import ( RANDOM_SEED, MODEL_CHECKPOINT_FILE )\t# type: ignore\n",
    "else:\n",
    "\t# We are running on Google Colab (import modules from the pulled repository stored in the project's directory)\n",
    "\tfrom computer_vision_project.src.scripts import models, datasets, training, evaluation, utils\t# type: ignore\n",
    "\tfrom computer_vision_project.src.scripts.utils import ( RANDOM_SEED, MODEL_CHECKPOINT_FILE )\t# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_4_'></a>[Configuration, Hyperparameters and Constants](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_1_'></a>[Random Seed](#toc0_)\n",
    "\n",
    "We **seed the random number generators** for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seeds for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_2_'></a>[Device Configuration](#toc0_)\n",
    "\n",
    "We **set the device** to GPU if available, otherwise we use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_3_'></a>[Database Constants](#toc0_)\n",
    "\n",
    "We **define the constants** used for the **database resources download** and the **dataset creation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of images to consider in the dataset (will be split into training, validation and test sets), set to -1 to use all the available images\n",
    "NUMBER_OF_IMAGES_IN_DB = 384\n",
    "\n",
    "# Minimum number of images per class\n",
    "# NOTE: For the \"ImageNet Reduced\" dataset, make sure to set MIN_IMAGES_PER_CLASS >= NUMBER_OF_IMAGES_IN_DB // 10\n",
    "MIN_IMAGES_PER_CLASS = 64\n",
    "\n",
    "# Percentage of images, for each class, to use for the image retrieval dataset (the remaining images will be used for the indexing dataset, i.e. will be added in the images database)\n",
    "IMAGE_RETRIEVAL_DB_PERCENTAGE = 0.75\n",
    "\n",
    "# Whether to shuffle the images before splitting them into training, validation and test sets (to make their IDs random)\n",
    "SHUFFLE_DB_IMAGES = False\n",
    "\n",
    "# MS COCO dataset constants\n",
    "# NOTE: Use only if \"DATASET\" is set to \"COCO\"\n",
    "COCO_DATA_YEAR = '2014'\t\t# '2014' or '2017'\n",
    "COCO_DATA_TYPE = 'val'\t\t# 'train' or 'val'\n",
    "COCO_DATA_CAPTIONS_FILE = f\"/annotations/captions_{COCO_DATA_TYPE}{COCO_DATA_YEAR}.json\"\t# Path of the annotations file inside the DATA_FOLDER\n",
    "CODO_DATA_INSTANCES_FILE = f\"/annotations/instances_{COCO_DATA_TYPE}{COCO_DATA_YEAR}.json\"\t# Path of the instances file inside the DATA_FOLDER\n",
    "MIN_COCO_IMAGE_CAPTIONS = 5\t\t# Minimum number of captions for an image to be added in the dataset\n",
    "\n",
    "# ImageNet dataset constants\n",
    "# NOTE: Use only if \"DATASET\" is set to \"imagenet\", \"imagenet_full\" or \"imagenet_reduced\"\n",
    "IMAGENET_DATA_FOLDER = \"data/imagenet\"\t# Path of the ImageNet dataset folder inside the DATA_FOLDER\n",
    "IMAGENET_DATA_CLASSES_FILE = f\"{IMAGENET_DATA_FOLDER}/classes.py\"\t# Path of the classes file inside the IMAGENET_DATA_FOLDER\n",
    "\n",
    "# Number of image patches per dimension (i.e. both vertically and horizontally, since images have a square aspect ratio)\n",
    "# NOTE: Only works for COCO and full ImageNet datasets (not for ImageNet small or ImageNet reduced, which always have 64x64 and 160x160 images respectively, thus a number of patches per dimension which depends on the chosen PATCH_SIZE)\n",
    "IMAGE_PATCHES_PER_DIMENSION = 16\n",
    "\n",
    "# If not enough square images are found, also accept images that have this max aspect difference (they will be cropped to a square aspect ratio later)\n",
    "# NOTE: Only works for COCO and full ImageNet datasets (not for ImageNet small or ImageNet reduced, which always have 64x64 and 160x160 images respectively, thus a fixed aspect ratio of 1)\n",
    "MAX_ASPECT_RATIO_TOLERANCE = 0.1 \t# Accept images that are 10% wider than they are tall (or vice versa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_4_'></a>[Models Hyperparameters](#toc0_)\n",
    "\n",
    "We then **define the constant** representing **hyperparameters** used for the **Vision Transformer** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the image patches\n",
    "IMAGE_PATCH_SIZE = 16\n",
    "\n",
    "# Dimensionality of the feature vectors given as input to the Vision Transformer\n",
    "TRANSFORMER_EMBEDDINGS_SIZE = 128\n",
    "\n",
    "# Dimensionality of the hidden layers in the feed-forward networks within the Transformer\n",
    "TRANSFORMER_FNN_HIDDEN_SIZE = 256\n",
    "\n",
    "# Number of heads to use in the Multi-Head Attention block\n",
    "# NOTE: must be a divisor of \"TRANSFORMER_EMBEDDINGS_SIZE\"\n",
    "TRANSFORMER_NUM_HEADS = 4\n",
    "\n",
    "# Number of layers to use in the Transformer (i.e. the number of Multi-Head Attention blocks and Feed-Forward networks)\n",
    "TRANSFORMER_NUM_LAYERS = 6\n",
    "\n",
    "# Number of epochs to train the Transformer model for the indexing and retrieval tasks\n",
    "TRANSFORMER_INDEXING_TRAINING_EPOCHS = 750\n",
    "TRANSFORMER_RETRIEVAL_TRAINING_EPOCHS = 75\n",
    "\n",
    "# Batch size for training the Transformer model\n",
    "TRANSFORMER_BATCH_SIZE = 32\n",
    "\n",
    "# DROPOUT rate to apply in the feed-forward network and on the input encoding\n",
    "TRANSFORMER_DROPOUT = 0.1\n",
    "\n",
    "# Learning rate for the optimizer\n",
    "TRANSFORMER_LEARNING_RATE = 0.0005\n",
    "\n",
    "# Whether to use learned positional encodings in the Transformer model (instead of the standard sinusoidal positional encodings)\n",
    "LEARN_POSITIONAL_ENCODINGS = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_5_'></a>[Evauation Constants](#toc0_)\n",
    "\n",
    "We also define the constants used for the evaluation of the various models (i.e. to compute the **Indexing Accuracy**, **Mean Average Precision** and the **Recall at K**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of IDs to retrieve for each query image to calculate the IA@K results\n",
    "IA_K = 5\n",
    "\n",
    "# Define the number of images K to retrieve for each query image and the number of queries N to calculate the mean average precision (MAP@K)\n",
    "MAP_K = 10\n",
    "MAP_N = 10\n",
    "\n",
    "# Define the number of images K to retrieve for each query image to calculate the Recall@K results\n",
    "RECALL_K = 100\n",
    "RECALL_N = 10\n",
    "\n",
    "# Whether to evaluate the models (i.e. compute the IA@K, MAP@K and Recall@K metrics for the trained models on the test datasets)\n",
    "EVALUATE_MODELS = True\n",
    "\n",
    "# Whether to print the debug information during the IA@K, MAP@K and Recall@K evaluation of the models\n",
    "PRINT_EVALUATION_DEBUG = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo Data Constants\n",
    "\n",
    "We define constants to use in case we want to test the model using demo data, which saves datasets and database creation time.\n",
    "\n",
    "To use demo data, set the `USE_DEMO_DATA` constant to `True` and specify the `DEMO_DATA_SIZE` constant to the desired demo database size.\n",
    "\n",
    "**‚ùóNOTE**: Using demo data reuires said data to be already available in the \"**./demo/{`DEMO_DATA_SIZE`}**\" directory.<br/>\n",
    "If no demo data is available, create the data by first running the Notebook with `USE_DEMO_DATA` set to `False` for the wanted demo data size, then copy the created datasets and database files inside the \"**./demo/{`DEMO_DATA_SIZE`}**\" directory and set `USE_DEMO_DATA` to `True` (for the defined `DEMO_DATA_SIZE`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Requires a demo data folder named \"demo/{DEMO_DATA_SIZE}/\" to be created first in the project's directory, containing the demo data\n",
    "\n",
    "# Whether to load demo data from the \"demo/\" folder (set to False to build the dataset from the COCO dataset using the above constants or to load its existing version)\n",
    "USE_DEMO_DATA = False\n",
    "\n",
    "# Number of images in the demo dataset (if LOAD_DEMO_DATA is set to True)\n",
    "DEMO_DATA_SIZE = 100\n",
    "\n",
    "# Folder containing the demo images and captions\n",
    "DEMO_FOLDER = f\"src/demo/{DEMO_DATA_SIZE}/\" if not RUNNING_IN_COLAB else f\"/content/computer_vision_project/src/demo/{DEMO_DATA_SIZE}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_6_'></a>[Other Constants](#toc0_)\n",
    "\n",
    "We ultimately define the constants used to determine whether to print examples from the various loaded datasets, where to save data and models, flags to enable/disable database rebuild/refresh and the loading of models checkpoints, ecc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Whether to print examples of the images and captions during the dataset creation\n",
    "PRINT_EXAMPLES = True\n",
    "\n",
    "# Define the data folder, onto which the various dictionaries, lists and other data will be saved\n",
    "DATA_FOLDER = \"src/data\" if not RUNNING_IN_COLAB else \"/content/data\"\n",
    "\n",
    "# Define the path to save models\n",
    "MODELS_FOLDER = \"src/models\" if not RUNNING_IN_COLAB else \"/content/models\"\n",
    "\n",
    "# Force the creation of the \"image_db\" images list, the JSON files for the datasets, ecc...\n",
    "FORCE_DICTIONARIES_CREATION = False\t\t# Set to false to try to load the dictionaries from the DATA_FOLDER if they exist\n",
    "\n",
    "# Whether to load model checkpoints (if they were already saved locally) or not\n",
    "LOAD_MODELS_CHECKPOINTS = True\n",
    "\n",
    "# Whether to save pytorch-specific datasets to the DATA_FOLDER (may lead to slower performances, higher ram/disk usage, and running out of available memory/RAM while trying to save the datasets)\n",
    "SAVE_PYTORCH_DATASETS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_7_'></a>[HuggingFace Tokens Configuration](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the **HuggingFace** API token to use for loading datasets from the HuggingFace Hub.\n",
    "\n",
    "**‚ö†Ô∏è Note**: Set the `HF_TOKEN` constant to contain your own HuggingFace API token only when using the **`IMAGENET_FULL`** dataset, as it requires authentication to download the dataset (unlike the **`IMAGENET_REDUCED`** and **`IMAGENET_TINY`** datasets, which can be downloaded without the need for an HuggingFace API token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HuggingFace API Access Token (for the ImageNet full dataset)\n",
    "# NOTE: Only needed in case \"DATASET\" is set to \"imagenet_full\"\n",
    "HF_TOKEN = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_8_'></a>[Weights & Biases Configuration](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the **Weights & Biases** API key to log the experiments.\n",
    "\n",
    "**‚ö†Ô∏è Note**: Copy and paste your own W&B API key into the `WANDB_API_KEY` constant to see logging results, or set the constant to an empty string to disable W&B logging (this won't plot training losses and accuracies over time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the WANDB_API_KEY (set to \"\" to disable W&B logging)\n",
    "# NOTE: leaving the WANDB_API_KEY to a value of None will throw an error\n",
    "WANDB_API_KEY = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We configure the **Weights & Biases** logger and API to track the experiments and the model's performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the project name to use for the W&B logging\n",
    "WANDB_PROJECT_NAME = \"vit-dsi-project\"\n",
    "\n",
    "# Define the wandb logger, api object, entity name and project name\n",
    "wandb_logger = None\n",
    "wandb_api = None\n",
    "wandb_entity = None\n",
    "wandb_project = None\n",
    "# Check if a W&B api key is provided\n",
    "if WANDB_API_KEY == None:\n",
    "\tprint(\"No W&B API key provided, please provide a valid key to use the W&B API or set the WANDB_API_KEY variable to an empty string to disable logging\")\n",
    "\traise ValueError(\"No W&B API key provided...\")\n",
    "elif WANDB_API_KEY != \"\":\n",
    "\t# Login to the W&B (Weights & Biases) API\n",
    "\twandb.login(key=WANDB_API_KEY, relogin=True)\n",
    "\t# Minimize the logging from the W&B (Weights & Biases) library\n",
    "\tos.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\tlogging.getLogger(\"wandb\").setLevel(logging.ERROR)\n",
    "\t# Initialize the W&B (Weights & Biases) loggger\n",
    "\twandb_logger = WandbLogger(log_model=\"all\", project=WANDB_PROJECT_NAME, name=\"- SEPARATOR -\")\n",
    "\t# Initialize the W&B (Weights & Biases) API\n",
    "\twandb_api = wandb.Api()\n",
    "\t# Get the W&B (Weights & Biases) entity name\n",
    "\twandb_entity = wandb_logger.experiment.entity\n",
    "\t# Get the W&B (Weights & Biases) project name\n",
    "\twandb_project = wandb_logger.experiment.project\n",
    "\t# Finish the \"separator\" experiment\n",
    "\twandb_logger.experiment.finish(quiet=True)\n",
    "\tprint(\"W&B API key provided, logging with W&B enabled.\")\n",
    "else:\n",
    "\tprint(\"No W&B API key provided, logging with W&B disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_4_1_9_'></a>[Local Files Folder Creation](#toc0_)\n",
    "\n",
    "We create the folders to store the data dictionaries and the model's checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders if they do not exist\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "\tprint(f\"Creating the data folder at '{DATA_FOLDER}'...\")\n",
    "\tos.makedirs(DATA_FOLDER)\n",
    "if not os.path.exists(MODELS_FOLDER):\n",
    "\tprint(f\"Creating the models folder at '{MODELS_FOLDER}'...\")\n",
    "\tos.makedirs(MODELS_FOLDER)\n",
    "if DATASET != DatasetType.COCO:\n",
    "\tif not os.path.exists(IMAGENET_DATA_FOLDER):\n",
    "\t\tprint(f\"Creating the ImageNet data folder at '{IMAGENET_DATA_FOLDER}'...\")\n",
    "\t\tos.makedirs(IMAGENET_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <a id='toc3_'></a>[üíæ Data Preparation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Download Data & Resources](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_1_'></a>[Download Datasets Associated Files and Libraries](#toc0_)\n",
    "\n",
    "Download all the files and resources needed for the project based on the chosen `DATASET` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary files and libraries for the datasets\n",
    "if DATASET == DatasetType.COCO:\n",
    "\t# Check if the annotation file for the COCO dataset exists, if it does not exist, download it\n",
    "\t!cd {DATA_FOLDER} && wget -nc http://images.cocodataset.org/annotations/annotations_trainval{COCO_DATA_YEAR}.zip\n",
    "\t!cd {DATA_FOLDER} && unzip -n annotations_trainval{COCO_DATA_YEAR}.zip\n",
    "else:\n",
    "\t# Download the classes.py file from https://huggingface.co/datasets/zh-plus/tiny-imagenet/resolve/main/classes.py\n",
    "\t!cd {IMAGENET_DATA_FOLDER} && wget -nc https://huggingface.co/datasets/zh-plus/tiny-imagenet/resolve/main/classes.py\n",
    "\tfrom data.imagenet import classes as imagenet_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_2_'></a>[Datasets Download](#toc0_)\n",
    "\n",
    "We download the final **ImageNet** or **MS COCO** dataset based on the chosen `DATASET` type and store the corresponding data in the `coco_captions`, `coco_instances` and `imagenet_data` object variables (only if `USE_DEMO_DATA` is set to `False`).\n",
    "\n",
    "We then print general information about the downloaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables to store the MS COCO or ImageNet datasets\n",
    "coco_captions = None\n",
    "coco_instances = None\n",
    "imagenet_data = None\n",
    "# Build the chosen datasets or load the demo dataset\n",
    "if not USE_DEMO_DATA:\n",
    "\tif DATASET == DatasetType.COCO:\n",
    "\t\t# Initialize the COCO api for captioning\n",
    "\t\tcoco_captions = COCO(f\"{DATA_FOLDER}{COCO_DATA_CAPTIONS_FILE}\")\n",
    "\t\t# Initialize the COCO api for object detection\n",
    "\t\tcoco_instances = COCO(f\"{DATA_FOLDER}{CODO_DATA_INSTANCES_FILE}\")\n",
    "\t\t# Show the COCO dataset info for the captioning task\n",
    "\t\tprint(\"\\nCOCO captioning dataset infos:\")\n",
    "\t\tcoco_captions.info()\n",
    "\t\t# Show the information for the captioning task\n",
    "\t\tprint(\"\\nCOCO captioning task infos:\")\n",
    "\t\tcoco_caps = coco_captions.dataset['annotations']\n",
    "\t\tprint(\"Number of images: \", len(coco_captions.getImgIds()))\n",
    "\t\tprint(\"Number of captions: \", len(coco_caps))\n",
    "\t\tprint(\"Number of average captions per image: \", len(coco_caps) / len(coco_captions.getImgIds()))\n",
    "\t\t# Show the COCO dataset info for the object detection task\n",
    "\t\tprint(\"\\nCOCO object detection dataset infos:\")\n",
    "\t\tcoco_instances.info()\n",
    "\t\t# Show the information for the object detection task\n",
    "\t\tprint(\"\\nCOCO object detection task infos:\")\n",
    "\t\tcoco_objs = coco_instances.dataset['annotations']\n",
    "\t\tprint(\"Number of images: \", len(coco_instances.getImgIds()))\n",
    "\t\tprint(\"Number of objects: \", len(coco_objs))\n",
    "\t\tprint(\"Number of categories: \", len(coco_instances.cats))\n",
    "\t\tprint(\"Categories:\")\n",
    "\t\tutils.print_json(coco_instances.cats)\n",
    "\telif DATASET == DatasetType.IMAGENET_TINY:\n",
    "\t\t# Get the ImageNet dataset using huggingface's tiny-imagenet-200 dataset at https://huggingface.co/datasets/zh-plus/tiny-imagenet\n",
    "\t\timagenet_data = hf_datasets.load_dataset(\"zh-plus/tiny-imagenet\")\n",
    "\t\t# Show the ImageNet dataset info\n",
    "\t\tprint(\"\\nImageNet Tiny dataset infos:\")\n",
    "\t\tprint(imagenet_data)\n",
    "\telif DATASET == DatasetType.IMAGENET_FULL:\n",
    "\t\t# Print an error message if the HF_TOKEN is not set\n",
    "\t\tif HF_TOKEN == None or HF_TOKEN == \"\":\n",
    "\t\t\tprint(\"Please set the \\\"HF_TOKEN\\\" constant to a valid HuggingFace API token to download the ImageNet full dataset or change dataset type using the \\\"DATASET\\\" constant.\")\n",
    "\t\t\traise ValueError(\"No HuggingFace API token provided...\")\n",
    "\t\t# Get the ImageNet dataset using huggingface's imagenet-1k dataset at https://huggingface.co/datasets/ILSVRC/imagenet-1k\n",
    "\t\timagenet_data = hf_datasets.load_dataset(\"ILSVRC/imagenet-1k\", token=HF_TOKEN, trust_remote_code=True)\n",
    "\t\t# Show the ImageNet dataset info\n",
    "\t\tprint(\"\\nImageNet Full dataset infos:\")\n",
    "\t\tprint(imagenet_data)\n",
    "\telif DATASET == DatasetType.IMAGENET_REDUCED:\n",
    "\t\t# Get the ImageNet dataset using huggingface's imagenette dataset at https://huggingface.co/datasets/frgfm/imagenette\n",
    "\t\timagenet_data = hf_datasets.load_dataset(\"frgfm/imagenette\", \"160px\")\n",
    "\t\t# Show the ImageNet dataset info\n",
    "\t\tprint(\"\\nImageNet Reduced dataset infos:\")\n",
    "\t\tprint(imagenet_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_3_'></a>[Datasets Examples](#toc0_)\n",
    "\n",
    "We print some examples of the images and their corresponding labels from the downloaded dataset (if the `PRINT_EXAMPLES` flag is set to `True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some examples from the dataset\n",
    "if not USE_DEMO_DATA and PRINT_EXAMPLES:\n",
    "\n",
    "\tif DATASET == DatasetType.COCO:\n",
    "\t\t# Print the first image object example\n",
    "\t\texample_image_index = 0\n",
    "\t\tprint(\"\\nImage object example: \")\n",
    "\t\timage_example = coco_captions.loadImgs(coco_caps[example_image_index]['image_id'])[0]\n",
    "\t\tutils.print_json(image_example, 2)\n",
    "\t\t# Print the actual image file\n",
    "\t\tprint(\"\\nActual image of the example (size: \" + str(image_example['width']) + \"x\" + str(image_example['height']) + \"):\")\n",
    "\t\turl = image_example['coco_url']\n",
    "\t\timage = io.imread(url)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(image)\n",
    "\t\tplt.show()\n",
    "\t\t# Downscale the image to the maximum allowed size in the model\n",
    "\t\timage_max_size = IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION\n",
    "\t\t# Crop the image to a square aspect ratio if it is not already square\n",
    "\t\tdownscaled_image = image\n",
    "\t\tif image_example['width'] > image_example['height']:\n",
    "\t\t\t# Image is wider than tall, crop the sides\n",
    "\t\t\tcrop_width = (image_example['width'] - image_example['height']) // 2\n",
    "\t\t\tdownscaled_image = image[:, crop_width:crop_width+image_example['height']]\n",
    "\t\telif image_example['height'] > image_example['width']:\n",
    "\t\t\t# Image is taller than wide, crop the top and bottom\n",
    "\t\t\tcrop_height = (image_example['height'] - image_example['width']) // 2\n",
    "\t\t\tdownscaled_image = image[crop_height:crop_height+image_example['width'], :]\n",
    "\t\t# Downscale the image to the maximum allowed size\n",
    "\t\tdownscaled_image = cv2.resize(downscaled_image, (image_max_size, image_max_size))\n",
    "\t\tprint(\"\\nDownscaled & cropped image of the example (size: \" + str(image_max_size) + \"x\" + str(image_max_size) + \"):\")\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(downscaled_image)\n",
    "\t\tplt.show()\n",
    "\t\t# Print the captions for the given image\n",
    "\t\tprint(\"\\nCaption examples for the given image: \")\n",
    "\t\tcaptions_for_image = coco_captions.loadAnns(coco_captions.getAnnIds(imgIds=image_example['id']))\n",
    "\t\tfor caption, j in zip(captions_for_image, range(len(captions_for_image))):\n",
    "\t\t\tprint(str(j+1) + \") \" + caption['caption'].strip())\n",
    "\t\t# Print the captioning object example\n",
    "\t\tprint(\"\\nFirst caption object example:\")\n",
    "\t\tutils.print_json(captions_for_image[0], 2)\n",
    "\t\t# Print information about the object detection task for the given image\n",
    "\t\tprint(\"\\nObject detection examples for the given image:\")\n",
    "\t\t# Get the object detection annotations for the given image\n",
    "\t\tannotations_for_image = coco_instances.loadAnns(coco_instances.getAnnIds(imgIds=image_example['id']))\n",
    "\t\tprint(\"List of the \" + str(len(annotations_for_image)) + \" object detection annotations for the given image (obtained using the 'coco_instances.loadAnns(image_annotation_id)' function):\")\n",
    "\t\tfor annotation, j in zip(annotations_for_image, range(len(annotations_for_image))):\n",
    "\t\t\tprint(\"\\n> Annotation \" + str(j+1) + \":\")\n",
    "\t\t\t# Print the annotation object\n",
    "\t\t\tutils.print_json(annotation, 2, truncate_large_lists=10)\n",
    "\telse:\n",
    "\t\t# Print the first image object example\n",
    "\t\texample_image_index = 0\n",
    "\t\tprint(\"\\nImage object example: \")\n",
    "\t\timage_example = imagenet_data['train'][example_image_index]\n",
    "\t\tprint(\"{\")\n",
    "\t\tprint(\"  'image':'\", image_example['image'])\n",
    "\t\tprint(\"  'label':\", image_example['label'])\n",
    "\t\tprint(\"}\")\n",
    "\t\t# Show the label name\n",
    "\t\tprint(\"\\nLabel of the example:\")\n",
    "\t\tlabel = image_example['label']\n",
    "\t\tlabel_id = imagenet_data['train'].features['label'].int2str(label)\n",
    "\t\tprint(\"Label index:\", label)\n",
    "\t\tprint(\"Label ID:\", label_id)\n",
    "\t\tprint(\"Label name:\", get_imagenet_class_name(label_id))\n",
    "\t\t# Show the PIL image of the example\n",
    "\t\tprint(\"\\nActual image of the example:\")\n",
    "\t\timage = image_example['image']\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Database Creation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_1_1_'></a>[Image Database Creation](#toc0_)\n",
    "\n",
    "We create the images database from the downloaded **ImageNet** and **MS COCO** datasets, and store it in the `images` list variable, a list of objects representing images with the corresponding associated information (IDs, class labels, image data, captions, ecc...).\n",
    "\n",
    "Alternatively, if the `USE_DEMO_DATA` flag is set to `True`, we load the **images demo data** from the `DEMO_FOLDER` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset of images for the training of the Vision Transformer model\n",
    "\n",
    "# List of image objects used for the training of the Vision Transformer model\n",
    "images = []\n",
    "\n",
    "# Flag used to detect whether the images_db list has been loaded or not\n",
    "loaded_images_db = False\n",
    "\n",
    "# Function that returns the list containing the images for the training of the Vision Transformer model\n",
    "def get_coco_images_db(number_of_images, process_images=True):\n",
    "\t'''\n",
    "\t\tBuilds a list of images for the training of the Vision Transformer model.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tnumber_of_images (int): The number of images to include in the dataset (search is stopped when the number of images is reached), use -1 to include all available images\n",
    "\t\t\tprocess_images (bool): Whether to process the images (i.e. retrieve actual image data, crop images and compupte their base64 encodings to add to the list)\n",
    "\t'''\n",
    "\t# Structure of the images\n",
    "\timages_list_object = {\n",
    "\t\t\"image_id\": \"\",\t\t\t# ID of the image (as found in the COCO dataset)\n",
    "\t\t\"image_url\": \"\",\t\t# URL of the image\n",
    "\t\t\"image_width\": 0,\t\t# The original image width\n",
    "\t\t\"image_height\": 0,\t\t# The original image height\n",
    "\t\t\"image_captions\": [],\t# List of captions for the image\n",
    "\t\t\"image_classes\": [\t\t# List of classes for the image (i.e. detected objects, in the order of area size)\n",
    "\t\t\t{\n",
    "\t\t\t\t\"class_id\": 0,\t\t# ID of the class (as found in the COCO dataset)\n",
    "\t\t\t\t\"class_name\": \"\",\t# Name of the class\n",
    "\t\t\t\t\"class_area\": 0,\t# Sum of the area of each instance of the class in the image\n",
    "\t\t\t\t\"class_count\": 0\t# Number of instances of the class in the image\n",
    "\t\t\t}\n",
    "\t\t],\t\n",
    "\t\t\"image_data\": \"\"\t\t# Base64 string of the image\n",
    "\t}\n",
    "\t# Get the image ids\n",
    "\timg_ids = coco_captions.getImgIds()\n",
    "\t# Get the images\n",
    "\timages = []\n",
    "\t# Function that returns a list of images with the given aspect ratio tolerance\n",
    "\tdef select_images_list(image_aspect_ratio_tolerance):\n",
    "\t\t# Get the images\n",
    "\t\tfor img_id in img_ids:\n",
    "\t\t\t# Get the image object\n",
    "\t\t\timg_obj = coco_captions.loadImgs(img_id)[0]\n",
    "\t\t\t# Check if the size of the image is square or within the aspect ratio tolerance\n",
    "\t\t\timage_aspect_ratio = img_obj['width'] / img_obj['height']\n",
    "\t\t\tif abs(image_aspect_ratio - 1) > image_aspect_ratio_tolerance:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Check if the image is already in the images list\n",
    "\t\t\tif any(img['image_id'] == img_obj['id'] for img in images):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Get the image url\n",
    "\t\t\timg_url = img_obj['coco_url']\n",
    "\t\t\t# Get the captions for the image\n",
    "\t\t\timg_captions = []\n",
    "\t\t\tcaptions = coco_captions.loadAnns(coco_captions.getAnnIds(imgIds=img_obj['id']))\n",
    "\t\t\tfor caption in captions:\n",
    "\t\t\t\tcaption_text = caption['caption'].strip()\n",
    "\t\t\t\tif len(caption_text) > 1:\n",
    "\t\t\t\t\timg_captions.append(caption_text)\n",
    "\t\t\t# Discard the image if the number of captions is less than the minimum\n",
    "\t\t\tif len(img_captions) < MIN_COCO_IMAGE_CAPTIONS:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Discard the image if it has no classes\n",
    "\t\t\tclasses = coco_instances.loadAnns(coco_instances.getAnnIds(imgIds=img_obj['id']))\n",
    "\t\t\tif len(classes) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Create a classes object with the fields: \"class_id\", \"class_name\", \"class_area\"\n",
    "\t\t\tclasses_obj = {}\n",
    "\t\t\tfor class_obj in classes:\n",
    "\t\t\t\tclass_id = class_obj['category_id']\n",
    "\t\t\t\tclass_name = coco_instances.cats[class_id]['name']\n",
    "\t\t\t\tclass_area = class_obj['area']\n",
    "\t\t\t\tif class_id not in classes_obj:\n",
    "\t\t\t\t\tclasses_obj[class_id] = {\n",
    "\t\t\t\t\t\t\"class_id\": class_id,\n",
    "\t\t\t\t\t\t\"class_name\": class_name,\n",
    "\t\t\t\t\t\t\"class_area\": class_area,\n",
    "\t\t\t\t\t\t\"class_count\": 1,\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tclasses_obj[class_id]['class_area'] += class_area\n",
    "\t\t\t\t\tclasses_obj[class_id]['class_count'] += 1\n",
    "\t\t\t# Convert the classes object to a list\n",
    "\t\t\tclasses_obj = list(classes_obj.values())\n",
    "\t\t\t# Sort the classes by area size\n",
    "\t\t\tclasses_obj = sorted(classes_obj, key=lambda x: x['class_area'], reverse=True)\n",
    "\t\t\t# Add the image to the images list\n",
    "\t\t\timages_list_object = {\n",
    "\t\t\t\t\"image_id\": img_obj['id'],\n",
    "\t\t\t\t\"image_url\": img_url,\n",
    "\t\t\t\t\"image_width\": img_obj['width'],\n",
    "\t\t\t\t\"image_height\": img_obj['height'],\n",
    "\t\t\t\t\"image_captions\": img_captions,\n",
    "\t\t\t\t\"image_classes\": classes_obj,\n",
    "\t\t\t\t\"image_data\": None # Will be filled later\n",
    "\t\t\t}\n",
    "\t\t\timages.append(images_list_object)\n",
    "\t\t\t# Break if the number of images is reached\n",
    "\t\t\tif number_of_images >= 1 and len(images) >= number_of_images:\n",
    "\t\t\t\tbreak\n",
    "\t\t# Return the images list\n",
    "\t\treturn images\n",
    "\tprint(\"Selecting images with a square aspect ratio...\")\n",
    "\t# Get the images that have a square aspect ratio first\n",
    "\timages = select_images_list(0)\n",
    "\t# Get the remaining images with the given aspect ratio tolerance\n",
    "\tif len(images) < number_of_images or number_of_images == -1:\n",
    "\t\tsquare_aspect_ratio_images = len(images)\n",
    "\t\tprint(\"> Found \" + str(square_aspect_ratio_images) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with a square aspect ratio, looking for the remaining images...\")\n",
    "\t\tprint(\"Looking for remaining images with an aspect ratio within a tolerance of \" + str(round(MAX_ASPECT_RATIO_TOLERANCE*100)) + \"% (either a \" + str(1 + MAX_ASPECT_RATIO_TOLERANCE) + \" aspect ratio or a \" + str(1 - MAX_ASPECT_RATIO_TOLERANCE) + \" aspect ratio)...\")\n",
    "\t\timages = select_images_list(MAX_ASPECT_RATIO_TOLERANCE)\n",
    "\t\tnon_square_aspect_ratio_images = len(images) - square_aspect_ratio_images\n",
    "\t\t# Print the number of images found\n",
    "\t\tprint(\"> Found \" + str(non_square_aspect_ratio_images) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" )  + \" more images with an aspect ratio within a tolerance of \" + str(round(MAX_ASPECT_RATIO_TOLERANCE*100)) + \"%.\")\n",
    "\telse:\n",
    "\t\tprint(\"> Found \" + str(len(images)) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with a square aspect ratio.\")\n",
    "\t# Print a message based on the number of images found\n",
    "\tif len(images) < number_of_images and number_of_images != -1:\n",
    "\t\tprint(\"WARNING: Could not find enough images with the required aspect ratio tolerance, only \" + str(len(images)) + \" / \" + str(number_of_images) + \" images found.\")\n",
    "\telse:\n",
    "\t\tprint(\"DONE: Found all \" + str(len(images)) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with the required aspect ratio tolerance.\")\n",
    "\t# Get all the image data\n",
    "\tif process_images:\n",
    "\t\tfor img in tqdm(images, desc=\"Processing images data (computing BASE64 images encoding)...\"):\n",
    "\t\t\timg[\"image_data\"] = utils.get_image_data_as_base64(img['image_url'], IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION)\n",
    "\t# Return the images list\n",
    "\treturn images\n",
    "\n",
    "# Function that returns the list containing the images for the training of the Vision Transformer model\n",
    "def get_imagenet_tiny_or_reduced_images_db(number_of_images, process_images=True):\n",
    "\t'''\n",
    "\t\tBuilds a list of images for the training of the Vision Transformer model.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tnumber_of_images (int): The number of images to include in the dataset (search is stopped when the number of images is reached), use -1 to include all available images\n",
    "\t\t\tprocess_images (bool): Whether to process the images (i.e. retrieve actual image data, crop images and compupte their base64 encodings to add to the list)\n",
    "\t'''\n",
    "\t# Structure of the images\n",
    "\timages_list_object = {\n",
    "\t\t\"image_id\": \"\",\t\t\t# ID of the image, i.e. index in the imaged_db list\n",
    "\t\t\"image_label_name\": \"\",\t# Name of the label of the image\n",
    "\t\t\"image_label_index\": 0,\t# Index of the label of the image (as found in huggingface's tiny imagenet dataset, i.e. the index of the class inside the dataset)\n",
    "\t\t\"image_label_id\": \"\",\t# ID of the label of the image (i.e. string with the form \"nX...X\" with X being integerrs, which is meant to be an identifier for the corresponding class in the original ImageNet database)\n",
    "\t\t\"image_data\": \"\"\t\t# Base64 string of the image\n",
    "\t}\n",
    "\t# Get the images\n",
    "\timages = []\n",
    "\t# Get the image ids\n",
    "\timg_ids = range(len(imagenet_data['train']))\n",
    "\t# Reference to the label ids\n",
    "\tlabel_ids = imagenet_data['train'].features['label']\n",
    "\t# Get the images, taking all of the images in the database and enriching them with the corresponding label names and ID\n",
    "\tfor img_id in img_ids:\n",
    "\t\t# Get the image object\n",
    "\t\timg_obj = imagenet_data['train'][img_id]\n",
    "\t\t# Get the image PIL object\n",
    "\t\timg_pil_obj = img_obj['image']\n",
    "\t\t# Convert the PIL object to a base64 string\n",
    "\t\timg_data = None\n",
    "\t\tif process_images:\n",
    "\t\t\timage_size = -1\n",
    "\t\t\tif DATASET == DatasetType.IMAGENET_TINY:\n",
    "\t\t\t\timage_size = 64\n",
    "\t\t\telif DATASET == DatasetType.IMAGENET_REDUCED:\n",
    "\t\t\t\timage_size = 160\n",
    "\t\t\timg_data = utils.get_image_data_as_base64(img_pil_obj, image_size)\n",
    "\t\t# Get the label of the image\n",
    "\t\timg_label = img_obj['label']\n",
    "\t\t# Get the label ID\n",
    "\t\timg_label_id = label_ids.int2str(img_label)\n",
    "\t\t# Get the label name\n",
    "\t\timg_label_name = get_imagenet_class_name(img_label_id)\n",
    "\t\t# Add the image to the images list\n",
    "\t\timages_list_object = {\n",
    "\t\t\t\"image_id\": img_id,\n",
    "\t\t\t\"image_label_name\": img_label_name,\n",
    "\t\t\t\"image_label_index\": img_label,\n",
    "\t\t\t\"image_label_id\": img_label_id,\n",
    "\t\t\t\"image_data\": img_data\n",
    "\t\t}\n",
    "\t\timages.append(images_list_object)\n",
    "\t\t# Break if the number of images is reached\n",
    "\t\tif number_of_images >= 1 and len(images) >= number_of_images:\n",
    "\t\t\tbreak\n",
    "\t# Return the images list\n",
    "\treturn images\n",
    "\n",
    "# Function that returns the list containing the images for the training of the Vision Transformer model\n",
    "def get_imagenet_full_images_db(number_of_images, process_images=True):\n",
    "\t'''\n",
    "\t\tBuilds a list of images for the training of the Vision Transformer model.\n",
    "\n",
    "\t\tParameters:\n",
    "\t\t\tnumber_of_images (int): The number of images to include in the dataset (search is stopped when the number of images is reached), use -1 to include all available images\n",
    "\t\t\tprocess_images (bool): Whether to process the images (i.e. retrieve actual image data, crop images and compupte their base64 encodings to add to the list)\n",
    "\t'''\n",
    "\t# Structure of the images\n",
    "\timages_list_object = {\n",
    "\t\t\"image_id\": \"\",\t\t\t# ID of the image, i.e. index in the imaged_db list\n",
    "\t\t\"image_label_name\": \"\",\t# Name of the label of the image\n",
    "\t\t\"image_label_index\": 0,\t# Index of the label of the image (as found in huggingface's tiny imagenet dataset, i.e. the index of the class inside the dataset)\n",
    "\t\t\"image_label_id\": \"\",\t# ID of the label of the image (i.e. string with the form \"nX...X\" with X being integerrs, which is meant to be an identifier for the corresponding class in the original ImageNet database)\n",
    "\t\t\"image_original_width\": \"\",\t\t# Original width of the image\n",
    "\t\t\"image_original_height\": \"\",\t# Original height of the image\n",
    "\t\t\"image_data\": \"\"\t\t# Base64 string of the image\n",
    "\t}\n",
    "\n",
    "\t# Get the images\n",
    "\timages_to_return = []\n",
    "\t# Get the image ids\n",
    "\timg_ids = range(len(imagenet_data['train']))\n",
    "\t# Reference to the label ids\n",
    "\tlabel_ids = imagenet_data['train'].features['label']\n",
    "\t# Function that returns a list of images with the given aspect ratio tolerance\n",
    "\tdef select_images_list(image_aspect_ratio_tolerance):\n",
    "\t\t# Get the images\n",
    "\t\tfor img_id in img_ids:\n",
    "\t\t\t# Get the image object\n",
    "\t\t\timg_obj = imagenet_data['train'][img_id]\n",
    "\t\t\t# Get the image PIL object\n",
    "\t\t\timg_pil_obj = img_obj['image']\n",
    "\t\t\t# Get the image width and height from the PIL object\n",
    "\t\t\timg_width, img_height = img_pil_obj.size\n",
    "\t\t\t# Check if the size of the image is square or within the aspect ratio tolerance\n",
    "\t\t\timage_aspect_ratio = img_width / img_height\n",
    "\t\t\tif abs(image_aspect_ratio - 1) > image_aspect_ratio_tolerance:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Check if the image is already in the images list\n",
    "\t\t\tif any(img['image_id'] == img_id for img in images_to_return):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# Convert the PIL object to a base64 string\n",
    "\t\t\timg_data = None\n",
    "\t\t\tif process_images:\n",
    "\t\t\t\timg_data = utils.get_image_data_as_base64(img_pil_obj, IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION)\n",
    "\t\t\t# Get the label of the image\n",
    "\t\t\timg_label = img_obj['label']\n",
    "\t\t\t# Get the label ID\n",
    "\t\t\timg_label_id = label_ids.int2str(img_label)\n",
    "\t\t\t# Get the label name\n",
    "\t\t\timg_label_name = get_imagenet_class_name(img_label_id)\n",
    "\t\t\t# Add the image to the images list\n",
    "\t\t\timages_list_object = {\n",
    "\t\t\t\t\"image_id\": img_id,\n",
    "\t\t\t\t\"image_label_name\": img_label_name,\n",
    "\t\t\t\t\"image_label_index\": img_label,\n",
    "\t\t\t\t\"image_label_id\": img_label_id,\n",
    "\t\t\t\t\"image_data\": img_data\n",
    "\t\t\t}\n",
    "\t\t\timages_to_return.append(images_list_object)\n",
    "\t\t\t# Break if the number of images is reached\n",
    "\t\t\tif number_of_images >= 1 and len(images_to_return) >= number_of_images:\n",
    "\t\t\t\tbreak\n",
    "\t\t# Return the images list\n",
    "\t\treturn images_to_return\n",
    "\tprint(\"Selecting images with a square aspect ratio...\")\n",
    "\t# Get the images that already have a square aspect ratio first\n",
    "\timages_to_return = select_images_list(0)\n",
    "\t# Get the remaining images with the given aspect ratio tolerance\n",
    "\tif len(images_to_return) < number_of_images or number_of_images == -1:\n",
    "\t\tsquare_aspect_ratio_images = len(images_to_return)\n",
    "\t\tprint(\"> Found \" + str(square_aspect_ratio_images) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with a square aspect ratio, looking for the remaining images...\")\n",
    "\t\tprint(\"Looking for remaining images with an aspect ratio within a tolerance of \" + str(round(MAX_ASPECT_RATIO_TOLERANCE*100)) + \"% (either a \" + str(1 + MAX_ASPECT_RATIO_TOLERANCE) + \" aspect ratio or a \" + str(1 - MAX_ASPECT_RATIO_TOLERANCE) + \" aspect ratio)...\")\n",
    "\t\timages_to_return = select_images_list(MAX_ASPECT_RATIO_TOLERANCE)\n",
    "\t\tnon_square_aspect_ratio_images = len(images_to_return) - square_aspect_ratio_images\n",
    "\t\t# Print the number of images found\n",
    "\t\tprint(\"> Found \" + str(non_square_aspect_ratio_images) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" )  + \" more images with an aspect ratio within a tolerance of \" + str(round(MAX_ASPECT_RATIO_TOLERANCE*100)) + \"%.\")\n",
    "\telse:\n",
    "\t\tprint(\"> Found \" + str(len(images_to_return)) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with a square aspect ratio.\")\n",
    "\t# Print a message based on the number of images found\n",
    "\tif len(images_to_return) < number_of_images and number_of_images != -1:\n",
    "\t\tprint(\"WARNING: Could not find enough images with the required aspect ratio tolerance, only \" + str(len(images_to_return)) + \" / \" + str(number_of_images) + \" images found.\")\n",
    "\telse:\n",
    "\t\tprint(\"DONE: Found all \" + str(len(images_to_return)) + (\" / \" + str(number_of_images) if number_of_images > 0 else \"\" ) + \" images with the required aspect ratio tolerance.\")\n",
    "\t# Return the images list\n",
    "\treturn images_to_return\n",
    "\n",
    "if not USE_DEMO_DATA:\n",
    "\t# Check if the images list should be rebuilt or loaded\n",
    "\timages_db_file = os.path.join(DATA_FOLDER, \"images_db.json\")\n",
    "\tif os.path.exists(images_db_file) and not FORCE_DICTIONARIES_CREATION:\n",
    "\t\twith open(images_db_file, 'r') as f:\n",
    "\t\t\timages = json.load(f)\n",
    "\t\tloaded_images_db = True\n",
    "\t\tprint(\"Loaded the images list from the file: \", images_db_file)\n",
    "\telse:\n",
    "\t\tif DATASET == DatasetType.COCO:\n",
    "\t\t\t# Initialize the images list\n",
    "\t\t\timages = get_coco_images_db(-1, False)\n",
    "\t\telif DATASET == DatasetType.IMAGENET_TINY or DATASET == DatasetType.IMAGENET_REDUCED:\n",
    "\t\t\t# Initialize the images list\n",
    "\t\t\timages = get_imagenet_tiny_or_reduced_images_db(-1, False)\n",
    "\t\telif DATASET == DatasetType.IMAGENET_FULL:\n",
    "\t\t\t# Initialize the images list\n",
    "\t\t\timages = get_imagenet_full_images_db(-1, False)\n",
    "\t\t# Save the images list to a JSON file\n",
    "\t\timages_db_file = os.path.join(DATA_FOLDER, \"images_db.json\")\n",
    "\t\tprint(\"Saving the images list to the file: \", images_db_file)\n",
    "\t\twith open(images_db_file, 'w') as f:\n",
    "\t\t\tjson.dump(images, f)\n",
    "\n",
    "\t# Print the final number of images in the dataset\n",
    "\tprint(\"\\nNumber of loaded images in the dataset: \" + str(len(images)) + (\"/\" + str(NUMBER_OF_IMAGES_IN_DB) if NUMBER_OF_IMAGES_IN_DB != -1 else \"\"))\n",
    "\t\n",
    "else:\n",
    "\t# Load the imaged_db.json file from the demo folder\n",
    "\timages_db_file = os.path.join(DEMO_FOLDER, \"images_db.json\")\n",
    "\tif os.path.exists(images_db_file):\n",
    "\t\twith open(images_db_file, 'r') as f:\n",
    "\t\t\timages = json.load(f)\n",
    "\t\tprint(\"Loaded the images list from the file: \", images_db_file)\n",
    "\telse:\n",
    "\t\tprint(\"ERROR: Could not load the demo images list from the file: \", images_db_file)\n",
    "\t\traise FileNotFoundError(\"Could not load the demo images list from the file: \" + images_db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_1_2_'></a>[Classes Definition](#toc0_)\n",
    "\n",
    "We populate the `classes` dictionary with the classes and their corresponding image IDs, considering images loaded from the **ImageNet** and **MS COCO** datasets and stored in the `images` list.\n",
    "\n",
    "Alternatively, if the `USE_DEMO_DATA` flag is set to `True`, we load the **classes demo data** from the `DEMO_FOLDER` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"classes\" dictionary with the classes found in the dataset and, for each of them, a list of the images in which they appear\n",
    "\n",
    "# Initialize the classes dictionary\n",
    "classes = {}\n",
    "\n",
    "# Flag used to detect whether the classes dictionary has been loaded or not\n",
    "loaded_classes = False\n",
    "\n",
    "if not USE_DEMO_DATA:\n",
    "\t# Function to get the classes dictionary from the images\n",
    "\tdef get_classes_dict():\n",
    "\t\t# Initialize the classes list\n",
    "\t\tclasses = {}\n",
    "\t\tif DATASET == DatasetType.COCO:\n",
    "\t\t\t# Get the classes from the images\n",
    "\t\t\tfor i in tqdm(range(len(images)), desc=\"Processing images for classes...\"):\n",
    "\t\t\t\timg = images[i]\n",
    "\t\t\t\tfor class_obj in img['image_classes']:\n",
    "\t\t\t\t\t# Get the class id\n",
    "\t\t\t\t\tclass_id = class_obj['class_id']\n",
    "\t\t\t\t\t# Add the class to the classes list if it does not exist\n",
    "\t\t\t\t\tif class_id not in classes.keys():\n",
    "\t\t\t\t\t\tclasses[class_id] = []\n",
    "\t\t\t\t\t# Add the image index to the class list\n",
    "\t\t\t\t\tclasses[class_id].append(i)\n",
    "\t\telse:\n",
    "\t\t\t# Get the classes from the images\n",
    "\t\t\tfor i in tqdm(range(len(images)), desc=\"Processing images for classes...\"):\n",
    "\t\t\t\timg = images[i]\n",
    "\t\t\t\t# Get the class id\n",
    "\t\t\t\tclass_id = img['image_label_index']\n",
    "\t\t\t\t# Add the class to the classes list if it does not exist\n",
    "\t\t\t\tif class_id not in classes.keys():\n",
    "\t\t\t\t\tclasses[class_id] = []\n",
    "\t\t\t\t# Add the image index to the class list\n",
    "\t\t\t\tclasses[class_id].append(i)\n",
    "\t\tprint(\"Created the classes list from the images with \" + str(len(classes)) + \" classes.\")\n",
    "\t\t# Discard the classes with less than the minimum number of images\n",
    "\t\tclasses = {k: v for k, v in classes.items() if len(v) >= MIN_IMAGES_PER_CLASS}\n",
    "\t\t# Sort classes by the number of images\n",
    "\t\tclasses = {k: v for k, v in sorted(classes.items(), key=lambda item: len(item[1]), reverse=True)}\n",
    "\t\tprint(\"Discarded the classes with less than \" + str(MIN_IMAGES_PER_CLASS) + \" images: \" + str(len(classes)) + \" / \" + str(len(classes.keys()) + len(classes)) + \" classes remaining.\")\n",
    "\t\t# Return the classes list\n",
    "\t\treturn classes\n",
    "\n",
    "\t# Get the classes dictionary if it already exists, otherwise create it\n",
    "\tclasses_file = os.path.join(DATA_FOLDER, \"classes.json\")\n",
    "\tif os.path.exists(classes_file) and not FORCE_DICTIONARIES_CREATION:\n",
    "\t\twith open(classes_file, 'r') as f:\n",
    "\t\t\tclasses = json.load(f)\n",
    "\t\tif len(classes) > 0:\n",
    "\t\t\tloaded_classes = True\n",
    "\t\t\tprint(\"Loaded the classes dictionary from the file: \", classes_file)\n",
    "\tif not loaded_classes:\n",
    "\t\tprint(\"Creating the classes dictionary from the images...\")\n",
    "\t\tclasses = get_classes_dict()\n",
    "\t\t# Save the classes dictionary to a JSON file\n",
    "\t\tprint(\"Saving the classes dictionary to the file: \", classes_file)\n",
    "\t\twith open(classes_file, 'w') as f:\n",
    "\t\t\tjson.dump(classes, f)\n",
    "else:\n",
    "\t# Load the classes.json file from the demo folder\n",
    "\tclasses_file = os.path.join(DEMO_FOLDER, \"classes.json\")\n",
    "\tif os.path.exists(classes_file):\n",
    "\t\twith open(classes_file, 'r') as f:\n",
    "\t\t\tclasses = json.load(f)\n",
    "\t\tprint(\"Loaded the classes dictionary from the file: \", classes_file)\n",
    "\telse:\n",
    "\t\tprint(\"ERROR: Could not load the demo classes dictionary from the file: \", classes_file)\n",
    "\t\traise FileNotFoundError(\"Could not load the demo classes dictionary from the file: \" + classes_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the entire `classes` dictionary to show the classes and their corresponding image IDs (truncated to the first 10 images of each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classes dictionary\n",
    "if PRINT_EXAMPLES:\n",
    "\tprint(\"\\nClasses dictionary:\")\n",
    "\tutils.print_json(classes, 2, truncate_large_lists=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_1_3_'></a>[Database Finalization](#toc0_)\n",
    "\n",
    "We update the `images` list to only contain the designated `NUMBER_OF_IMAGES_IN_DB` images, making sure we include at least `MIN_IMAGES_PER_CLASS` images for each class.\n",
    "If `SHUFFLE_IMAGES_DB` is set to `True`, we shuffle the images in the database to assign them random IDs.\n",
    "\n",
    "We also update the `classes` dictionary to only include a number of classes compatible with the defined `NUMBER_OF_IMAGES_IN_DB` and `MIN_IMAGES_PER_CLASS` constants.\n",
    "\n",
    "Ultimately, we compute the **BASE64** encoding of image data for each image in the `images` list to be stored as a string, locally, in the corresponding JSON files in the `DATA_FOLDER` directory.\n",
    "\n",
    "We finally print information about the final number of images and classes included in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the images in the images DB to finally only include the images that have the classes in the classes list, with MIN_IMAGES_PER_CLASS images per class, and to populate the images list with the base64 encoding of the images\n",
    "\n",
    "if not USE_DEMO_DATA and not loaded_images_db and not loaded_classes:\n",
    "\n",
    "\t# Function to update the images list to only include the images that have the classes in the classes list\n",
    "\tdef update_images_db_based_on_classes(max_images):\n",
    "\t\t# Number of classes to maintain the designated number of images\n",
    "\t\tclasses_count = math.ceil(max_images / MIN_IMAGES_PER_CLASS)\n",
    "\t\t# Initialize the new images list\n",
    "\t\tnew_images_db = []\n",
    "\t\timages_db_ids_map = {}\n",
    "\t\t# Get the classes to maintain the designated number of images\n",
    "\t\tclasses_to_maintain = list(classes.keys())[:classes_count]\n",
    "\t\t# Create a new classes list with the classes found in the new images list\n",
    "\t\tnew_classes = {}\n",
    "\t\t# Get the images to maintain the designated number of images\n",
    "\t\tif DATASET == DatasetType.COCO:\n",
    "\t\t\t# Select the first \"max_images\" images that have the classes in the classes list\n",
    "\t\t\tfor i in tqdm(range(len(images_to_return)), desc=\"Processing images for classes...\"):\n",
    "\t\t\t\timg = images_to_return[i]\n",
    "\t\t\t\t# Check if the image has any of the classes to maintain\n",
    "\t\t\t\tmantain_image = any(class_obj['class_id'] in classes_to_maintain for class_obj in img['image_classes'])\n",
    "\t\t\t\tif mantain_image:\n",
    "\t\t\t\t\tnew_images_db.append(img)\n",
    "\t\t\t\t\timages_db_ids_map[i] = len(new_images_db) - 1\n",
    "\t\t\t\t# Break if the number of images is reached\n",
    "\t\t\t\tif len(new_images_db) >= max_images:\n",
    "\t\t\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\t# Select the first images of each class to maintain until we reach the maximum number of images \"max_images\"\n",
    "\t\t\timages_per_class = MIN_IMAGES_PER_CLASS\n",
    "\t\t\tfor class_id in tqdm(classes_to_maintain, desc=\"Processing images for classes...\"):\n",
    "\t\t\t\t# Get the images of the class\n",
    "\t\t\t\timages_of_class = classes[class_id]\n",
    "\t\t\t\t# Get the number of images to maintain for the class\n",
    "\t\t\t\timages_to_maintain = min(images_per_class, len(images_of_class))\n",
    "\t\t\t\t# Add the images to the new images list\n",
    "\t\t\t\tfor i in range(images_to_maintain):\n",
    "\t\t\t\t\timg = images_to_return[images_of_class[i]]\n",
    "\t\t\t\t\tnew_images_db.append(img)\n",
    "\t\t\t\t\timages_db_ids_map[images_of_class[i]] = len(new_images_db) - 1\n",
    "\t\t\t\t# Break if the number of images is reached\n",
    "\t\t\t\tif len(new_images_db) >= max_images:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t# Get the classes from the classes to maintain\n",
    "\t\tfor class_id in classes_to_maintain:\n",
    "\t\t\t# Remove any image index that is not in the new images list\n",
    "\t\t\tnew_classes[class_id] = [images_db_ids_map[i] for i in classes[class_id] if i in images_db_ids_map]\n",
    "\t\t# Sort the classes by the number of images\n",
    "\t\tnew_classes = { k: v for k, v in sorted(new_classes.items(), key=lambda item: len(item[1]), reverse=True) }\n",
    "\t\t# Shuffle the images in the database, if needed, and make the classes list consistent with the new images list \"IDs\", i.e. the index of the image in the new images list\n",
    "\t\tif SHUFFLE_DB_IMAGES:\n",
    "\t\t\t# Get a list of all IDs of images in the new images list, i.e. a list of indexes to then use for shuffling\n",
    "\t\t\timages_ids = list(range(len(new_images_db)))\n",
    "\t\t\t# Shuffle the images IDs\n",
    "\t\t\trandom.shuffle(images_ids)\n",
    "\t\t\t# Shuffle the images list\n",
    "\t\t\tshuffled_images_db = [new_images_db[i] for i in images_ids]\n",
    "\t\t\t# Update the classes list to use the new image IDs\n",
    "\t\t\tfor class_id in new_classes.keys():\n",
    "\t\t\t\tnew_classes[class_id] = [images_ids.index(i) for i in new_classes[class_id]]\n",
    "\t\t\t# Update the new images list\n",
    "\t\t\tnew_images_db = shuffled_images_db\n",
    "\t\t# Return the new images list and the new classes list\n",
    "\t\treturn new_images_db, new_classes\n",
    "\n",
    "\t# Update the images list to only include the images that have the classes in the classes list\n",
    "\tif not loaded_images_db or not loaded_classes:\n",
    "\t\t# Update the images list to only include the images that have the classes in the classes list\n",
    "\t\tmax_images = NUMBER_OF_IMAGES_IN_DB if NUMBER_OF_IMAGES_IN_DB != -1 else len(images)\n",
    "\t\tprint(\"\\nUpdating the images list to only include the \" + str(len(classes)) + \" classes with at least \" + str(MIN_IMAGES_PER_CLASS) + \" images, not exceeding \" + str(max_images) + \" images...\")\n",
    "\t\timages_to_return, classes = update_images_db_based_on_classes(max_images)\n",
    "\t\tprint(\"DONE: Updated the images list, now containing \" + str(len(images_to_return)) + \" images.\")\n",
    "\t\tprint(\"> Final number of classes in the dataset: \" + str(len(classes)))\n",
    "\t\t# Update the images list to include the base64 encoding of the images\n",
    "\t\tprint(\"Computing the BASE64 images encoding for the images list...\")\n",
    "\t\tmax_image_size = -1\n",
    "\t\tif DATASET == DatasetType.COCO:\n",
    "\t\t\tmax_image_size = IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION\n",
    "\t\t\tfor img in tqdm(images_to_return, desc=\"Processing images data (computing BASE64 images encoding)...\"):\n",
    "\t\t\t\timg[\"image_data\"] = utils.get_image_data_as_base64(img['image_url'], max_image_size)\n",
    "\t\telse:\n",
    "\t\t\tmax_image_size = -1\n",
    "\t\t\tif DATASET == DatasetType.IMAGENET_TINY:\n",
    "\t\t\t\tmax_image_size = 64\n",
    "\t\t\telif DATASET == DatasetType.IMAGENET_FULL:\n",
    "\t\t\t\tmax_image_size = IMAGE_PATCH_SIZE * IMAGE_PATCHES_PER_DIMENSION\n",
    "\t\t\telif DATASET == DatasetType.IMAGENET_REDUCED:\n",
    "\t\t\t\tmax_image_size = 160\n",
    "\t\t\tfor img in tqdm(range(len(images_to_return)), desc=\"Processing images data (computing BASE64 images encoding)...\"):\n",
    "\t\t\t\tif images_to_return[img][\"image_data\"] is not None:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\timagenet_image_id = images_to_return[img][\"image_id\"]\n",
    "\t\t\t\tpil_img_obj = imagenet_data['train'][imagenet_image_id]['image']\n",
    "\t\t\t\timages_to_return[img][\"image_data\"] = utils.get_image_data_as_base64(pil_img_obj, max_image_size)\n",
    "\t\tprint(\"DONE: Computed the BASE64 images encoding for the images list.\")\n",
    "\t\t# Save the updated images list to a JSON file\n",
    "\t\timages_db_file = os.path.join(DATA_FOLDER, \"images_db.json\")\n",
    "\t\tprint(\"Saving the updated images list to the file: \", images_db_file)\n",
    "\t\twith open(images_db_file, 'w') as f:\n",
    "\t\t\tjson.dump(images_to_return, f)\n",
    "\n",
    "actual_image_patches_per_dimension = -1\n",
    "if DATASET == DatasetType.COCO or DATASET == DatasetType.IMAGENET_FULL:\n",
    "\tactual_image_patches_per_dimension = IMAGE_PATCHES_PER_DIMENSION\n",
    "elif DATASET == DatasetType.IMAGENET_TINY:\n",
    "\tactual_image_patches_per_dimension = 64 // IMAGE_PATCH_SIZE\n",
    "elif DATASET == DatasetType.IMAGENET_REDUCED:\n",
    "\tactual_image_patches_per_dimension = 160 // IMAGE_PATCH_SIZE\n",
    "\n",
    "# Print the final number of images in the dataset\n",
    "print(\"\\nNumber of loaded images in the dataset: \" + str(len(images_to_return)))\n",
    "print(\"Number of classes in the dataset: \" + str(len(classes)))\t\n",
    "print(\"\\nImage patch size: \" + str(IMAGE_PATCH_SIZE))\n",
    "print(\"Number of image patches per dimension: \" + str(actual_image_patches_per_dimension))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the final `classes` dictionary and `images` list (if the `PRINT_EXAMPLES` flag is set to `True`).\n",
    "\n",
    "We also print an example image and its representation from the point of view of the **Vision Transformer** model (i.e. divided into **patches** of size `IMAGE_PATCH_SIZE`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRINT_EXAMPLES:\n",
    "\t# Print the classes list\n",
    "\tif DATASET == DatasetType.COCO:\n",
    "\t\ttotal_classes = -1 if USE_DEMO_DATA else len(coco_instances.cats)\n",
    "\t\tprint(\"\\nClasses list sorted by number of images (\" + str(len(classes)) + \" classes out of \" + (str(total_classes) if total_classes != -1 else \"???\") + \" total MS COCO classes):\")\n",
    "\t\tutils.print_json(classes, 2, truncate_large_lists=10)\n",
    "\telse:\n",
    "\t\ttotal_classes = -1 if USE_DEMO_DATA else imagenet_data['train'].features['label'].num_classes\n",
    "\t\tprint(\"\\nClasses list sorted by number of images (\" + str(len(classes)) + \" classes out of \" + (str(total_classes) if total_classes != -1 else \"???\") + \" total ImageNet classes):\")\n",
    "\t\tutils.print_json(classes, 2, truncate_large_lists=10)\n",
    "\t# Print the final images list\n",
    "\tprint(\"\\nAll images list:\")\n",
    "\tutils.print_json(images_to_return, 2, truncate_large_lists=5)\n",
    "\t# Print all the sinigle classes that appear in the list\n",
    "\tif DATASET == DatasetType.COCO:\n",
    "\t\tprint(\"\\nAll classes in DB list (\" + str(len(classes)) + \" classes):\")\n",
    "\t\tactual_db_classes = list(set([class_obj['class_name'] for img in images_to_return for class_obj in img['image_classes']]))\n",
    "\t\tprint(actual_db_classes)\n",
    "\telse:\n",
    "\t\tactual_db_classes = list(set([class_name for class_name in [image[\"image_label_name\"] for image in images_to_return]]))\n",
    "\t\tprint(\"\\nAll classes in DB list (\" + str(len(actual_db_classes)) + \" classes out of \" + str(len(classes)) + \" expected):\")\n",
    "\t\tprint(actual_db_classes)\n",
    "\t# Print the first image object example\n",
    "\texample_image_index = -1\n",
    "\tprint(\"Image object example: \")\n",
    "\tutils.print_json(images_to_return[example_image_index], 2)\n",
    "\t# Print the actual image file\n",
    "\timage_b64_string = None\n",
    "\tif images_to_return[example_image_index]['image_data'] != None:\n",
    "\t\timage_b64_string = images_to_return[example_image_index]['image_data']\n",
    "\telse:\n",
    "\t\tif DATASET == DatasetType.COCO:\n",
    "\t\t\timage_b64_string = utils.get_image_data_as_base64(images_to_return[example_image_index]['image_url'], actual_image_patches_per_dimension)\n",
    "\t\telse:\n",
    "\t\t\timage_b64_string = utils.get_image_data_as_base64(imagenet_data['train'][example_image_index]['image'], actual_image_patches_per_dimension)\n",
    "\timage = utils.get_image_from_b64_string(image_b64_string)\n",
    "\tif DATASET == DatasetType.COCO or DATASET == DatasetType.IMAGENET_FULL:\n",
    "\t\timage_width = images_to_return[example_image_index]['image_width']\n",
    "\t\timage_height = images_to_return[example_image_index]['image_height']\n",
    "\t\tprint(\"\\nActual image of the example (original size: \" + str(image_width) + \"x\" + str(image_height) + \" | downsampled size: \" + str(image.shape[1]) + \"x\" + str(image.shape[0]) + \"):\")\n",
    "\telif DATASET == DatasetType.IMAGENET_TINY:\n",
    "\t\tprint(\"\\nActual image of the example (original size: 64x64 | downsampled size: \" + str(image.shape[1]) + \"x\" + str(image.shape[0]) + \"):\")\n",
    "\telif DATASET == DatasetType.IMAGENET_REDUCED:\n",
    "\t\tprint(\"\\nActual image of the example (original size: 160x160 | downsampled size: \" + str(image.shape[1]) + \"x\" + str(image.shape[0]) + \"):\")\n",
    "\tplt.axis('off')\n",
    "\tplt.imshow(image)\n",
    "\tplt.show()\n",
    "\t# Print how the Transformer model sees the image\n",
    "\tprint(\"\\nHow the Transformer model sees the image (downsampled size: \" + str(image.shape[1]) + \"x\" + str(image.shape[0]) + \"):\")\n",
    "\t# Divide the image into smaller images representing the patches, then show the separated patches\n",
    "\timage_patches = []\n",
    "\tfor i in range(actual_image_patches_per_dimension):\n",
    "\t\tfor j in range(actual_image_patches_per_dimension):\n",
    "\t\t\timage_patch = image[i*IMAGE_PATCH_SIZE:(i+1)*IMAGE_PATCH_SIZE, j*IMAGE_PATCH_SIZE:(j+1)*IMAGE_PATCH_SIZE]\n",
    "\t\t\timage_patches.append(image_patch)\n",
    "\t# Show the image patches in a grid\n",
    "\tif actual_image_patches_per_dimension > 1:\n",
    "\t\tfig, axs = plt.subplots(actual_image_patches_per_dimension, actual_image_patches_per_dimension, figsize=(10, 10))\n",
    "\t\tfor i in range(actual_image_patches_per_dimension):\n",
    "\t\t\tfor j in range(actual_image_patches_per_dimension):\n",
    "\t\t\t\taxs[i, j].axis('off')\n",
    "\t\t\t\taxs[i, j].imshow(image_patches[i*actual_image_patches_per_dimension+j])\n",
    "\telse:\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(image_patches[0])\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Indexing & Retrieval Datasets](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_3_1_1_'></a>[Training Datasets Creation](#toc0_)\n",
    "\n",
    "We split the original `images` list into separate datasets for the **indexing** and **retrieval** tasks.\n",
    "\n",
    "Two different datasets are created:\n",
    "\n",
    "- **Indexing Dataset**: A dataset to train the ViT model for the **indexing task**, in which the model learns to generate image IDs starting from **images' patches** as source sequences.\n",
    "\n",
    "   Items of the dataset have the form **`(db_image, image_id)`** where `db_image` is an image to consider being included in our indexed database, and `image_id` is the ID of the image in the indexed database (either \"clustered\" or \"random\" based on the `SHUFFLE_IMAGES_DB` flag).\n",
    "\n",
    "   The indexed database images are stored in an **`images_db_indexing`** list variable, containing image objects corresponding to the images in the indexed database.\n",
    "\n",
    "- **Retrieval Dataset**: A dataset to train the ViT model for the **retrieval task**, in which the model learns to generate image IDs starting from **image queries' patches** as source sequences.\n",
    "\n",
    "   Items of the dataset have the form **`(query_image, image_id)`** where `query_image` is an image that is NOT included in our indexed database, and `image_id` is the ID of an image in the indexed database that is considered relevant to the query image.\n",
    "\n",
    "   The query images are stored in an **`images_db_retrieval`** dictionary variable, where the keys are the image IDs of queries found in the `images` list, and the values are lists of relevant image IDs from the indexed database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the images list into a list for the indexing dataset (i.e. images in the database) and a list for the image retrieval dataset (i.e. similar images to retrieve images in the DB)\n",
    "\n",
    "# List of images for the indexing and image retrieval datasets\n",
    "images_db_indexing = []\t# List of images for the indexing dataset\n",
    "images_db_retrieval = {} # Dictionary containing image IDs of images NOT in the indexing dataset as keys and the list of similar images in the indexing dataset as values\n",
    "\n",
    "# Dictionary for mapping image IDs in the images_db (i.e. indexes of its items) into the IDs of the images that are only in the indexing dataset (i.e. their indexes in the images_db_indexing list)\n",
    "map_images_db_to_indexing_db = {}\n",
    "# Dictionary for the reverse mapping (from the indexing dataset to the original dataset)\n",
    "map_indexing_db_to_images_db = {}\n",
    "\n",
    "# Check if the indexing and image retrieval datasets should be rebuilt or loaded\n",
    "can_load_dictionaries = False\n",
    "if not USE_DEMO_DATA:\n",
    "\timages_db_indexing_file = os.path.join(DATA_FOLDER, \"images_db_indexing.json\")\n",
    "\timages_db_image_retrieval_file = os.path.join(DATA_FOLDER, \"images_db_image_retrieval.json\")\n",
    "\tif os.path.exists(images_db_indexing_file) and os.path.exists(images_db_image_retrieval_file) and not FORCE_DICTIONARIES_CREATION:\n",
    "\t\tcan_load_dictionaries = True\n",
    "\n",
    "# Build or load the indexing and image retrieval datasets\n",
    "if not USE_DEMO_DATA:\n",
    "\tif not can_load_dictionaries:\n",
    "\t\t# Create the indexing and image retrieval datasets from the images list\n",
    "\t\tfor class_id_or_index in classes.keys():\n",
    "\t\t\tclass_obj = classes[class_id_or_index]\n",
    "\t\t\tindexing_number = int(len(class_obj) * (1 - IMAGE_RETRIEVAL_DB_PERCENTAGE))\n",
    "\t\t\tpredicted_similar_images = []\n",
    "\t\t\tfor j in range(len(class_obj)):\n",
    "\t\t\t\tis_in_db = j < indexing_number\n",
    "\t\t\t\timg_id = class_obj[j]\n",
    "\t\t\t\t# Get the image object\n",
    "\t\t\t\timg = images_to_return[img_id]\n",
    "\t\t\t\tif is_in_db:\n",
    "\t\t\t\t\t# Add the image to the indexing dataset as an image object (containing various image infos)\n",
    "\t\t\t\t\timages_db_indexing.append(img)\n",
    "\t\t\t\t\t# Add the image to the similar images list\n",
    "\t\t\t\t\tpredicted_similar_images.append(img_id)\n",
    "\t\t\t\t\t# Store the remapping of the image IDs for the image retrieval dataset (i.e. the index of the image in the indexing dataset)\n",
    "\t\t\t\t\t#\tNOTE: this is done so that we can consider ONLY images in our indexing datasets as our final images in the dataset\n",
    "\t\t\t\t\tmap_images_db_to_indexing_db[img_id] = len(images_db_indexing) - 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# Add the image to the image retrieval dataset (as the index of the image in the indexing dataset, not the original image ID)\n",
    "\t\t\t\t\timages_db_retrieval[img_id] = [ map_images_db_to_indexing_db[i] for i in predicted_similar_images ]\n",
    "\t\t# Store the reverse mapping of the image IDs for the image retrieval dataset\n",
    "\t\tmap_indexing_db_to_images_db = { v: k for k, v in map_images_db_to_indexing_db.items() }\n",
    "\t\t# Save the indexing and image retrieval datasets to JSON files, along with both the remapped image IDs dictionaries\n",
    "\t\timages_db_indexing_file = os.path.join(DATA_FOLDER, \"images_db_indexing.json\")\n",
    "\t\tprint(\"Saving the images list for the indexing dataset to the file: \", images_db_indexing_file)\n",
    "\t\twith open(images_db_indexing_file, 'w') as f:\n",
    "\t\t\tjson.dump(images_db_indexing, f)\n",
    "\t\timages_db_image_retrieval_file = os.path.join(DATA_FOLDER, \"images_db_image_retrieval.json\")\n",
    "\t\tprint(\"Saving the images list for the image retrieval dataset to the file: \", images_db_image_retrieval_file)\n",
    "\t\twith open(images_db_image_retrieval_file, 'w') as f:\n",
    "\t\t\tjson.dump(images_db_retrieval, f)\n",
    "\t\tremapped_indexing_image_ids_file = os.path.join(DATA_FOLDER, \"remapped_indexing_image_ids.json\")\n",
    "\t\tprint(\"Saving the remapped image IDs for the image retrieval dataset to the file: \", remapped_indexing_image_ids_file)\n",
    "\t\twith open(remapped_indexing_image_ids_file, 'w') as f:\n",
    "\t\t\tjson.dump(map_images_db_to_indexing_db, f)\n",
    "\t\tremapped_indexing_image_ids_reverse_file = os.path.join(DATA_FOLDER, \"remapped_indexing_image_ids_reverse.json\")\n",
    "\t\tprint(\"Saving the reverse remapped image IDs for the image retrieval dataset to the file: \", remapped_indexing_image_ids_reverse_file)\n",
    "\t\twith open(remapped_indexing_image_ids_reverse_file, 'w') as f:\n",
    "\t\t\tjson.dump(map_indexing_db_to_images_db, f)\n",
    "\t\tprint(\"DONE: Created the images list for the indexing and image retrieval datasets.\")\n",
    "\telse:\n",
    "\t\t# Load the indexing and image retrieval datasets from the JSON files\n",
    "\t\twith open(images_db_indexing_file, 'r') as f:\n",
    "\t\t\timages_db_indexing = json.load(f)\n",
    "\t\tprint(\"Loaded \" + str(len(images_db_indexing)) + \" images for the indexing dataset from the file: \", images_db_indexing_file)\n",
    "\t\twith open(images_db_image_retrieval_file, 'r') as f:\n",
    "\t\t\timages_db_retrieval = json.load(f)\n",
    "\t\tprint(\"Loaded \" + str(len(images_db_retrieval)) + \" images for the image retrieval dataset from the file: \", images_db_image_retrieval_file)\n",
    "\t\t# Load the remapped image IDs for the image retrieval dataset\n",
    "\t\tremapped_indexing_image_ids_file = os.path.join(DATA_FOLDER, \"remapped_indexing_image_ids.json\")\n",
    "\t\twith open(remapped_indexing_image_ids_file, 'r') as f:\n",
    "\t\t\tmap_images_db_to_indexing_db = json.load(f)\n",
    "\t\tprint(\"Loaded the remapped image IDs for the image retrieval dataset from the file: \", remapped_indexing_image_ids_file)\n",
    "\t\tremapped_indexing_image_ids_reverse_file = os.path.join(DATA_FOLDER, \"remapped_indexing_image_ids_reverse.json\")\n",
    "\t\twith open(remapped_indexing_image_ids_reverse_file, 'r') as f:\n",
    "\t\t\tmap_indexing_db_to_images_db = json.load(f)\n",
    "\t\tprint(\"Loaded the reverse remapped image IDs for the image retrieval dataset from the file: \", remapped_indexing_image_ids_reverse_file)\n",
    "else:\n",
    "\t# Load the images_db_indexing.json and images_db_image_retrieval.json files from the demo folder\n",
    "\timages_db_indexing_file = os.path.join(DEMO_FOLDER, \"images_db_indexing.json\")\n",
    "\timages_db_image_retrieval_file = os.path.join(DEMO_FOLDER, \"images_db_image_retrieval.json\")\n",
    "\tif os.path.exists(images_db_indexing_file):\n",
    "\t\twith open(images_db_indexing_file, 'r') as f:\n",
    "\t\t\timages_db_indexing = json.load(f)\n",
    "\t\tprint(\"Loaded \" + str(len(images_db_indexing)) + \" images for the indexing dataset from the file: \", images_db_indexing_file)\n",
    "\telse:\n",
    "\t\tprint(\"ERROR: Could not load the demo images list for the indexing dataset from the file: \", images_db_indexing_file)\n",
    "\t\traise FileNotFoundError(\"Could not load the demo images list for the indexing dataset from the file: \" + images_db_indexing_file)\n",
    "\tif os.path.exists(images_db_image_retrieval_file):\n",
    "\t\twith open(images_db_image_retrieval_file, 'r') as f:\n",
    "\t\t\timages_db_retrieval = json.load(f)\n",
    "\t\tprint(\"Loaded \" + str(len(images_db_retrieval)) + \" images for the image retrieval dataset from the file: \", images_db_image_retrieval_file)\n",
    "\telse:\n",
    "\t\tprint(\"ERROR: Could not load the demo images list for the image retrieval dataset from the file: \", images_db_image_retrieval_file)\n",
    "\t\traise FileNotFoundError(\"Could not load the demo images list for the image retrieval dataset from the file: \" + images_db_image_retrieval_file)\n",
    "\n",
    "# Compute the max length of the image IDS (we consider the index of the image in the \"images_db\" as the image ID)\n",
    "max_image_id_length = len(str(len(images_db_indexing)))\n",
    "# Number of output tokens for the encoded image IDs (the 10 digits [0-9] plus the 3 special tokens, i.e. end of sequence, padding, start of sequence)\n",
    "output_tokens = 10 + 3\n",
    "\n",
    "# Print the final number of images in the datasets\n",
    "print(\"\\nNumber of images in the indexing dataset: \" + str(len(images_db_indexing)))\n",
    "print(\"Image IDs max length: \" + str(max_image_id_length))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_3_1_2_'></a>[Transformer Datasets Creation](#toc0_)\n",
    "\n",
    "Starting from the `images_db_indexing` and `images_db_retrieval` datasets, we create the **PyTorch Datasets** to be used for training the **Vision Transformer** model:\n",
    "\n",
    "- **`TransformerIndexingDataset`**: Items of the dataset have the form **`(db_image, image_id)`** where `db_image` is a 3D RGB tensor representing an image included in our indexed database, and `image_id` is the encoded representation of the ID of the image (starting from a \"Begin of Sequence\" token\", followed by a sequnce of digits representing the image ID, padded to the given `max_image_id_length` using the corresponding \"Padding\" token, and ending with an \"End of Sequence\" token, with each token being represented as a vector).\n",
    "\n",
    "- **`TransformerRetrievalDataset`**: Items of the dataset have the form **`(query_image, image_id)`** where `query_image` is a 3D RGB tensor representing an image that is NOT included in our indexed database, and `image_id` is the encoded representation of the ID of an image in the indexed database that is considered relevant to the query image (encoded with the same approach as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the indexing and image retrieval datasets to be used for training the Vision Transformer model\n",
    "\n",
    "# Paths of the file in which the PyTorch datasets will be stored or from which they will be loaded\n",
    "transformer_indexing_dataset_file = None\n",
    "transformer_image_retrieval_dataset_file = None\n",
    "if SAVE_PYTORCH_DATASETS:\n",
    "\ttransformer_indexing_dataset_file = (DATA_FOLDER + \"/\" if not USE_DEMO_DATA else DEMO_FOLDER) + \"transformer_indexing_dataset.json\"\n",
    "\ttransformer_image_retrieval_dataset_file = (DATA_FOLDER + \"/\" if not USE_DEMO_DATA else DEMO_FOLDER) + \"transformer_image_retrieval_dataset.json\"\n",
    "\n",
    "# Build the Transformer Indexing Database for training the vision transformer\n",
    "transformer_indexing_dataset = datasets.TransformerIndexingDataset(\n",
    "\timages=images_db_indexing,\n",
    "\tpatch_size=IMAGE_PATCH_SIZE,\n",
    "\timg_patches=actual_image_patches_per_dimension,\n",
    "\timg_id_max_length=max_image_id_length,\n",
    "\tdataset_file_path=transformer_indexing_dataset_file,\n",
    "\tforce_dataset_rebuild=FORCE_DICTIONARIES_CREATION and not USE_DEMO_DATA\n",
    ")\n",
    "\n",
    "# Build the Transformer Image Retrieval Database for training the vision transformer\n",
    "transformer_image_retrieval_dataset = datasets.TransformerRetrievalDataset(\n",
    "\tall_images=images_to_return,\n",
    "\tsimilar_images=images_db_retrieval,\n",
    "\tpatch_size=IMAGE_PATCH_SIZE,\n",
    "\timg_patches=actual_image_patches_per_dimension,\n",
    "\timg_id_max_length=max_image_id_length,\n",
    "\tdataset_file_path=transformer_image_retrieval_dataset_file,\n",
    "\tforce_dataset_rebuild=FORCE_DICTIONARIES_CREATION and not USE_DEMO_DATA,\n",
    "\timages_db_to_indexing_db=map_images_db_to_indexing_db,\n",
    "\tindexing_db_to_images_db=map_indexing_db_to_images_db\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print some **examples** of the **Indexing Dataset** and the **Retrieval Dataset** to visualize the data (if the `PRINT_EXAMPLES` flag is set to `True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRINT_EXAMPLES:\n",
    "\t# Print the first example from the Transformer Indexing Dataset\n",
    "\texample_index = random.randint(0, len(transformer_indexing_dataset)-1)\n",
    "\tprint(\"Example from the Transformer Indexing Dataset:\")\n",
    "\tprint(\"<encoded_image, encoded_image_id> tuple:\")\n",
    "\tprint(transformer_indexing_dataset[example_index])\n",
    "\t# Print the first example from the Transformer Image Retrieval Dataset\n",
    "\texample_index = random.randint(0, len(transformer_image_retrieval_dataset)-1)\n",
    "\tprint(\"\\nExample from the Transformer Image Retrieval Dataset:\")\n",
    "\tprint(\"<encoded_image, encoded_similar_image_id> tuple:\")\n",
    "\tprint(transformer_image_retrieval_dataset[example_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[üõçÔ∏è Bag of Visual Words Model](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_1_'></a>[BoVW Model Initialization](#toc0_)\n",
    "\n",
    "We initialize our baseline **Bag of Visual Words** model, based on the **SIFT** detector and **K-Means** clustering algorithm.<br/>\n",
    "Unlike the **Vision Transformer** model, the **BoVW** model does not require training (uses no machine learning), as it is based on a fixed pipeline of feature extraction, clustering, and histogram generation.\n",
    "\n",
    "The steps to initialize the BoVW model are as follows:\n",
    "\n",
    "1. **Feature Extraction**: We extract the **SIFT** features from all the images in the indexed database, represented as a list of **keypoints** with their corresponding vector **descriptors**;\n",
    "\n",
    "2. **Clustering**: We cluster the extracted features using the **K-Means** algorithm (we use `K`=150) to create a **visual words vocabulary** of `K` visual words (K-Means centroids);\n",
    "\n",
    "3. **Histogram Generation**: We generate the **histograms** for each image in the indexed database, representing the frequency of SIFT features assigned to each visual word in the vocabulary (BoVW representation of the indexed database);\n",
    "\n",
    "4. **Database Indexing**: We store the BoVW histograms in a dictionary, where the keys are the image IDs and the values are the corresponding BoVW representations (histograms);\n",
    "\n",
    "At **inference** time, given an image query `Q`:\n",
    "\n",
    "1. We extract the **SIFT** features from the query image `Q` as a list of **keypoints** with their corresponding vector **descriptors**;\n",
    "\n",
    "2. We assign each feature to the closest visual word in the vocabulary, generating a histogram representing the frequency of SIFT features assigned to each visual word (BoVW representation of the query image `Q`);\n",
    "\n",
    "3. We iterate over all the BoVW representations of the indexed database images, computing the **cosine similarity** between the BoVW representation of the query image `Q` and each indexed database image (**index-then-retrieve** image retrieval pipeline);\n",
    "\n",
    "4. We return the **sorted list of image IDs** based on the computed cosine similarities.\n",
    "\n",
    "**NOTE:** The **BoVW** model uses grayscale images, so we convert the RGB images to grayscale before extracting the SIFT features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BoVW model for the image retrieval dataset\n",
    "print(\"\\nCreating a BoVW model for the image retrieval dataset...\")\n",
    "BOVW_model = models.BoVW(\n",
    "\tall_images=images_to_return,\n",
    "\tindexed_images=images_db_indexing,\n",
    "\tkmeans_clusters=150\n",
    ")\n",
    "print(\"DONE: Created the BoVW model for the image retrieval dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[ü§ñ Vision Transformer Model (DSI approach)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_1_'></a>[ViT Model Initialization](#toc0_)\n",
    "\n",
    "We initialize the **Vision Transformer** model for the **Differentiable Search Index (DSI)** approach with the given hyperparameters and configuration (set by the various constants defined in the **constants** section of this Notebook).\n",
    "\n",
    "The model is based on the **Sequence to Sequence** architecture, where the **encoder** processes the image patches and the **decoder** generates the image IDs.\n",
    "\n",
    "Images are divided into **patches** of size `IMAGE_PATCH_SIZE`, each represented as a **3D RGB tensor**.\n",
    "\n",
    "Image IDs are represented as a sequence of digits, starting from a **\"Begin of Sequence\"** token, followed by the sequence of digits representing the image ID, padded to the given `max_image_id_length` using the corresponding **\"Padding\"** token, and ending with an **\"End of Sequence\"** token: each of these tokens is represented as a vector.\n",
    "\n",
    "The model is trained using:\n",
    "- The **TransformerIndexingDataset** to first learn to output image IDs given indexed images as input using \"**Masked Attention**\";\n",
    "\n",
    "- The **TransformerRetrievalDataset** to learn to output image IDs given query images as input using the knowledge acquired during the indexing task.\n",
    "\n",
    "At **inference** time, given an image query `Q`, the model uses an \"auto-regressive\" approach to generate the image IDs based on the patches of the query image `Q` to which the **Begin of Sequence** token is appended, digit by digit, returning the **logits** (non-normalized probabilities) for each of the 10 possible digits at each step.\n",
    "\n",
    "If a `WANDB_API_KEY` is provided, the model also logs the training and validation losses and accuracies to the **Weights & Biases** dashboard and shows the graphs of the training and validation losses and accuracies over time at the end of the training process, for both the indexing and retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer_model():\n",
    "\t''' Auxiliary function to train (or load checkpoints), show training results, and evaluate the transformer model of the given type '''\n",
    "\t\n",
    "\tdsi_transformer_args = {\n",
    "\t\t# Dimensionality of the input feature vectors to the Transformer (i.e. the size of the embeddings)\n",
    "\t\t\"embed_dim\": TRANSFORMER_EMBEDDINGS_SIZE, \n",
    "\t\t# Dimensionality of the hidden layer in the feed-forward networks within the Transformer\n",
    "\t\t\"hidden_dim\": TRANSFORMER_FNN_HIDDEN_SIZE, \n",
    "\t\t# Number of channels of the input images (e.g. 3 for RGB, 1 for grayscale, ecc...)\n",
    "\t\t\"num_channels\": 3,\t\n",
    "\t\t# Number of heads to use in the Multi-Head Attention block (should be a divisor of the embed_dim)\n",
    "\t\t\"num_heads\": TRANSFORMER_NUM_HEADS,\t\n",
    "\t\t# Number of layers to use in the Transformer\n",
    "\t\t\"num_layers\": TRANSFORMER_NUM_LAYERS,\n",
    "\t\t# Size of each batch\n",
    "\t\t\"batch_size\": TRANSFORMER_BATCH_SIZE,\n",
    "\t\t# Number of classes to predict (in my case, since I give an image with, concatenated, the N digits of the image ID, the num_classes is the number of possible digits of the image IDs, hence 10+3, including the special tokens)\n",
    "\t\t\"num_classes\": output_tokens,\n",
    "\t\t# Size of the image patches\n",
    "\t\t\"patch_size\": IMAGE_PATCH_SIZE,\n",
    "\t\t# Maximum number of patches an image can have\n",
    "\t\t\"num_patches\": actual_image_patches_per_dimension * actual_image_patches_per_dimension,\n",
    "\t\t# Maximum length of the image IDs\n",
    "\t\t\"img_id_max_length\": max_image_id_length,\n",
    "\t\t# Special tokens for the image IDs\n",
    "\t\t\"img_id_start_token\": 10,\n",
    "\t\t\"img_id_end_token\": 12,\n",
    "\t\t\"img_id_padding_token\": 11,\n",
    "\t\t# learn positional encodings\n",
    "\t\t\"learn_positional_encodings\": LEARN_POSITIONAL_ENCODINGS,\n",
    "\t\t# Dropout to apply in the feed-forward network and on the input encoding\n",
    "\t\t\"dropout\": TRANSFORMER_DROPOUT,\n",
    "\t\t# Learning rate for the optimizer\n",
    "\t\t\"learning_rate\": TRANSFORMER_LEARNING_RATE,\n",
    "\t\t# Other parameters (useful for keeping track of the model's training variations in the logging files and in the W&B dashboard)\n",
    "\t\t\"dataset_size\": NUMBER_OF_IMAGES_IN_DB,\n",
    "\t\t\"images_per_class\": MIN_IMAGES_PER_CLASS,\n",
    "\t\t\"retrieval_db_percentage\": IMAGE_RETRIEVAL_DB_PERCENTAGE,\n",
    "\t\t\"shuffle_db_images\": SHUFFLE_DB_IMAGES,\n",
    "\t\t\"patches_per_dimension\": actual_image_patches_per_dimension,\n",
    "\t\t\"max_aspect_ratio_tolerance\": MAX_ASPECT_RATIO_TOLERANCE\n",
    "\t}\n",
    "\n",
    "\t# Initialize transformer model\n",
    "\ttransformer_model = models.DSI_VisionTransformer(**dsi_transformer_args)\n",
    "\ttransformer_model.to(device)\n",
    "\n",
    "\t# Model's checkpoint file\n",
    "\tmodel_checkpoint_file = MODELS_FOLDER + \"/\" + \"ViT_\" + MODEL_CHECKPOINT_FILE\n",
    "\n",
    "\t# Train the model or load its saved checkpoint\n",
    "\ttransformer_retrieval_test_set = None\n",
    "\ttransformer_retrieval_test_set_file = DATA_FOLDER + f\"/ViT_transformer_retrieval_test_set.json\"\n",
    "\tif LOAD_MODELS_CHECKPOINTS and os.path.exists(model_checkpoint_file):\n",
    "\t\t# Load the saved models checkpoint\n",
    "\t\tprint(\"A checkpoint for the model exist, loading the saved model checkpoint...\")\n",
    "\t\ttransformer_model = models.DSI_VisionTransformer.load_from_checkpoint(model_checkpoint_file, **dsi_transformer_args)\n",
    "\t\tprint(\"Model checkpoint loaded.\")\n",
    "\t\t# Load the transformer retrieval test set from the JSON file\n",
    "\t\tprint(\"Loading the transformer retrieval test set from the JSON file...\")\n",
    "\t\twith open(transformer_retrieval_test_set_file, \"r\") as transformer_retrieval_test_set_file:\n",
    "\t\t\ttransformer_retrieval_test_set = json.load(transformer_retrieval_test_set_file)\n",
    "\t\tprint(\"Transformer retrieval test set loaded.\")\n",
    "\telse:\n",
    "\t\t# Create 2 loggers for the transformer model (one for the indexing task and one for the retrieval task)\n",
    "\t\ttransformer_loggers = None\n",
    "\t\tif wandb_api is not None:\n",
    "\t\t\ttransformer_wandb_logger_indexing = WandbLogger(log_model=\"all\", project=wandb_project, name=\"ViT (Indexing)\")\n",
    "\t\t\ttransformer_wandb_logger_retrieval = WandbLogger(log_model=\"all\", project=wandb_project, name=\"ViT (Retrieval)\")\n",
    "\t\t\ttransformer_loggers = [transformer_wandb_logger_indexing, transformer_wandb_logger_retrieval]\n",
    "\t\t# Train the transformer model for the indexing task\n",
    "\t\ttransformer_training_infos = training.train_transformer(\n",
    "\t\t\ttransformer_indexing_dataset=transformer_indexing_dataset,\n",
    "\t\t\ttransformer_retrieval_dataset=transformer_image_retrieval_dataset,\n",
    "\t\t\ttransformer_model=transformer_model,\n",
    "\t\t\tmax_epochs_list=[TRANSFORMER_INDEXING_TRAINING_EPOCHS, TRANSFORMER_RETRIEVAL_TRAINING_EPOCHS],\n",
    "\t\t\tbatch_size=transformer_model.hparams.batch_size,\n",
    "\t\t\tindexing_split_ratios=(1.0, 0.0),\n",
    "\t\t\tretrieval_split_ratios=(0.9, 0.05, 0.05),\n",
    "\t\t\tlogger=transformer_loggers,\n",
    "\t\t\tsave_path=model_checkpoint_file\n",
    "\t\t)\n",
    "\t\t# Show the wandb training run's dashboard\n",
    "\t\tif wandb_api is not None:\n",
    "\t\t\tindexing_run_id = transformer_training_infos[\"run_ids\"][\"indexing\"]\n",
    "\t\t\tif indexing_run_id is not None:\n",
    "\t\t\t\tprint(f\"Indexing training results for the ViT model:\")\n",
    "\t\t\t\tindexing_run_object: wandb_run.Run = wandb_api.run(f\"{wandb_entity}/{wandb_project}/{indexing_run_id}\")\n",
    "\t\t\t\tindexing_run_object.display(height=1000)\n",
    "\t\t\tretrieval_run_id = transformer_training_infos[\"run_ids\"][\"retrieval\"]\n",
    "\t\t\tif retrieval_run_id is not None:\n",
    "\t\t\t\tprint(f\"Retrieval training results for the ViT model:\")\n",
    "\t\t\t\tretrieval_run_object: wandb_run.Run = wandb_api.run(f\"{wandb_entity}/{wandb_project}/{retrieval_run_id}\")\n",
    "\t\t\t\tretrieval_run_object.display(height=1000)\n",
    "\t\t# Save the generated transformer retrieval test set to the JSON file\n",
    "\t\tprint(\"Saving the transformer retrieval test set to the JSON file...\")\n",
    "\t\tretrieval_test_dataset = transformer_training_infos[\"retrieval\"][\"test\"]\n",
    "\t\ttransformer_retrieval_test_set = {\n",
    "\t\t\t\"images\": [],\n",
    "\t\t\t\"relevant_ids\": []\n",
    "\t\t}\n",
    "\t\tretrieval_test_dataset_length = len(retrieval_test_dataset)\n",
    "\t\tfor i in range(retrieval_test_dataset_length):\n",
    "\t\t\tencoded_img, img_id = retrieval_test_dataset[i]\n",
    "\t\t\ttransformer_retrieval_test_set[\"images\"].append(encoded_img.tolist())\n",
    "\t\t\ttransformer_retrieval_test_set[\"relevant_ids\"].append(img_id.tolist())\n",
    "\t\twith open(transformer_retrieval_test_set_file, \"w\") as transformer_retrieval_test_set_file:\n",
    "\t\t\tjson.dump(transformer_retrieval_test_set, transformer_retrieval_test_set_file)\n",
    "\n",
    "\treturn transformer_model, transformer_retrieval_test_set\n",
    "\n",
    "# Train the vision transformer model\n",
    "vision_transformer, transformer_retrieval_test_set = train_transformer_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[üìà Evaluation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_1_'></a>[BoVW Model Evaluation](#toc0_)\n",
    "\n",
    "We evaluate the baseline **Bag of Visual Words** model by computing the **Indexing Accuracy**, **Mean Average Precision (MAP)** and **Recall at K** metrics on the images of our test dataset.\n",
    "\n",
    "We then print the **evaluation results** for the BoVW model.\n",
    "\n",
    "**‚ùóNote**: The baseline BoVW model evaluation is performed using the **index-then-retrieve** pipeline, and for this reason, the **Indexing Accuracy** of the model is not computed, since the BoVW model does not generate image IDs directly, but rather retrieves images based on the cosine similarity between the BoVW representation of the query image and the BoVW representations of the indexed database images, providing a **sorted list of image IDs** based on the computed cosine similarities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the BoVW model and print its evaluation results\n",
    "if EVALUATE_MODELS:\n",
    "\tprint(\"\\nEvaluating the BoVW model...\")\n",
    "\tbovw_indexing_accuracy = None\t# BoVW does not provide DB indexing\n",
    "\tbovw_retrieval_map_k = evaluation.compute_mean_average_precision_at_k(\n",
    "\t\timages_to_return, classes,\n",
    "\t\tk_results=MAP_K, n_images=MAP_N,\n",
    "\t\tprint_debug=PRINT_EVALUATION_DEBUG,\n",
    "\t\tmodel=BOVW_model, retrieval_dataset=transformer_image_retrieval_dataset, retrieval_test_set=transformer_retrieval_test_set\n",
    "\t)\n",
    "\tbovw_retrieval_recall_k = evaluation.compute_recall_at_k(\n",
    "\t\timages_to_return, classes,\n",
    "\t\tk_results=RECALL_K, n_images=RECALL_N,\n",
    "\t\tprint_debug=PRINT_EVALUATION_DEBUG,\n",
    "\t\tmodel=BOVW_model, retrieval_dataset=transformer_image_retrieval_dataset, retrieval_test_set=transformer_retrieval_test_set\n",
    "\t)\n",
    "\tutils.print_model_evaluation_results(bovw_indexing_accuracy, bovw_retrieval_map_k, bovw_retrieval_recall_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print some **examples** of the **retrieved images** for a given query image using the BoVW model (if the `PRINT_EXAMPLES` flag is set to `True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the BoVW model by providing, as input, some random images from the image retrieval dataset\n",
    "num_tests = 3\n",
    "num_results = 5\n",
    "if PRINT_EXAMPLES:\n",
    "\t# Get the images to test the BoVW model\n",
    "\timages_to_test = []\n",
    "\tfor i in range(num_tests):\n",
    "\t\t# Get a random image from the image retrieval dataset\n",
    "\t\trandom_image_id = random.choice(list(images_db_retrieval.keys()))\n",
    "\t\t# Get the image object\n",
    "\t\trandom_image = images_to_return[random_image_id]\n",
    "\t\t# Add the image to the list of images to test\n",
    "\t\timages_to_test.append({\n",
    "\t\t\t\"id\": random_image_id,\n",
    "\t\t\t\"img_obj\": random_image\n",
    "\t\t})\n",
    "\t# Test the BoVW model\n",
    "\tfor i in range(len(images_to_test)):\n",
    "\t\t# Get the image to test\n",
    "\t\ttest_img_infos = images_to_test[i]\n",
    "\t\ttest_img_id = test_img_infos[\"id\"]\n",
    "\t\ttest_img_obj = test_img_infos[\"img_obj\"]\n",
    "\t\t# Actual similar images\n",
    "\t\tactual_similar_images = images_db_retrieval[test_img_id]\n",
    "\t\t# Get the predictiono from the BoVW model\n",
    "\t\tpredicted_similar_images = BOVW_model.get_similar_images(test_img_obj, num_results)\n",
    "\t\t# Print information about the results\n",
    "\t\tprint(\"\\nImage \" + str(i+1) + \" to test (ID: \" + str(test_img_id) + \") | Class: \\\"\" + test_img_obj['image_label_name'] + \"\\\"\")\n",
    "\t\tcompact_print = True\n",
    "\t\tif compact_print:\n",
    "\t\t\tprint(\"> Actual similar images:\\n  \",end=\"\")\n",
    "\t\t\tprint([img_obj[\"image_id\"] for img_obj in [images_db_indexing[img_id] for img_id in actual_similar_images]])\n",
    "\t\t\tprint(\"> Predicted similar images (BoVW model)\\n  \",end=\"\")\n",
    "\t\t\tprint([img_obj[\"image_id\"] for img_obj in predicted_similar_images])\n",
    "\t\telse:\n",
    "\t\t\tprint(\"> Actual similar images:\")\n",
    "\t\t\tfor j in range(len(actual_similar_images)):\n",
    "\t\t\t\tsimilar_image_obj = images_db_indexing[actual_similar_images[j]]\n",
    "\t\t\t\timage_index = map_indexing_db_to_images_db[actual_similar_images[j]]\n",
    "\t\t\t\tprint(\"  - Image #\", j, \" (ID: \", image_index, \") | Class: \\\"\", similar_image_obj['image_label_name'],\"\\\"\", sep=\"\")\n",
    "\t\t\tprint(\"> Predicted similar images (BoVW model):\")\n",
    "\t\t\tfor j in range(len(predicted_similar_images)):\n",
    "\t\t\t\tsimilar_image_obj = predicted_similar_images[j]\n",
    "\t\t\t\tprint(\"  - Image #\", j, \" (ImageNet ID: \", similar_image_obj[\"image_id\"], \") | Class: \\\"\", similar_image_obj['label'],\"\\\"\", sep=\"\")\n",
    "\t\tprint(\"> Correct predictions: \", len(set(actual_similar_images).intersection(set([img[\"image_id\"] for img in predicted_similar_images]))), \" / \", num_results, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_2_'></a>[ViT Model Evaluation](#toc0_)\n",
    "\n",
    "We evaluate the **Vision Transformer** model for the **Differentiable Search Index (DSI)** approach by computing the **Indexing Accuracy**, **Mean Average Precision (MAP)** and **Recall at K** metrics on the images of our test dataset.\n",
    "\n",
    "We then print the **evaluation results** for the ViT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the transformer model and print its evaluation results\n",
    "if EVALUATE_MODELS:\n",
    "\tvision_transformer.eval()\n",
    "\tvision_transformer.to(device)\n",
    "\ttransformer_indexing_accuracy = evaluation.compute_indexing_accuracy(\n",
    "\t\ttransformer_indexing_dataset, transformer_image_retrieval_dataset,\n",
    "\t\tmodel=vision_transformer, \n",
    "\t\tk_results=IA_K,\n",
    "\t\tprint_debug=PRINT_EVALUATION_DEBUG\n",
    "\t)\n",
    "\ttransformer_retrieval_map_k = evaluation.compute_mean_average_precision_at_k(\n",
    "\t\timages_to_return, classes,\n",
    "\t\tk_results=MAP_K, n_images=MAP_N,\n",
    "\t\tprint_debug=PRINT_EVALUATION_DEBUG,\n",
    "\t\tmodel=vision_transformer, retrieval_dataset=transformer_image_retrieval_dataset, retrieval_test_set=transformer_retrieval_test_set\n",
    "\t)\n",
    "\ttransformer_retrieval_recall_k = evaluation.compute_recall_at_k(\n",
    "\t\timages_to_return, classes,\n",
    "\t\tk_results=RECALL_K, n_images=RECALL_N,\n",
    "\t\tprint_debug=PRINT_EVALUATION_DEBUG,\n",
    "\t\tmodel=vision_transformer, retrieval_dataset=transformer_image_retrieval_dataset, retrieval_test_set=transformer_retrieval_test_set\n",
    "\t)\n",
    "\t# Print the evaluation results of the Transformer model\n",
    "\tutils.print_model_evaluation_results(transformer_indexing_accuracy, transformer_retrieval_map_k, transformer_retrieval_recall_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print some **examples** of the **retrieved images** for a given query image using the ViT model (if the `PRINT_EXAMPLES` flag is set to `True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained traansformer model by generating the top k image IDs for an image in the retrieval dataset\n",
    "if vision_transformer is not None:\n",
    "\t# Total images to test\n",
    "\ttests = 3\n",
    "\t# Set the model to evaluation mode\n",
    "\tvision_transformer.eval()\n",
    "\tvision_transformer.to(device)\n",
    "\t# Test the model\n",
    "\tif tests >= 1:\n",
    "\t\tfor i in range(tests):\n",
    "\t\t\t# Get the image to test (as a tensor of the encoded image)\n",
    "\t\t\ttest_img_retrieval_db_index = i * (len(transformer_image_retrieval_dataset) // tests)\n",
    "\t\t\ttest_image = transformer_image_retrieval_dataset[test_img_retrieval_db_index][0]\n",
    "\t\t\ttest_img_db_index = transformer_image_retrieval_dataset.original_ids[test_img_retrieval_db_index]\n",
    "\t\t\tprint(\"\\nImage ID to test: \", test_img_retrieval_db_index , \" (Original ID: \", test_img_db_index, \")\",sep=\"\")\n",
    "\t\t\tutils.print_json(images_to_return[test_img_db_index])\n",
    "\t\t\t# relevant_image_id = [transformer_image_retrieval_dataset.decode_image_id(encoded_id) for encoded_id in transformer_image_retrieval_dataset[image_id_to_test][1]]\n",
    "\t\t\tsimilar_image_obj_id = -1\n",
    "\t\t\tif isinstance(list(transformer_image_retrieval_dataset.similar_images.keys())[0], int):\n",
    "\t\t\t\tsimilar_image_obj_id = test_img_db_index\n",
    "\t\t\telif isinstance(list(transformer_image_retrieval_dataset.similar_images.keys())[0], str):\n",
    "\t\t\t\tsimilar_image_obj_id = str(test_img_db_index)\n",
    "\t\t\trelevant_image_indexing_ids = transformer_image_retrieval_dataset.similar_images[similar_image_obj_id]\n",
    "\t\t\trelevant_image_db_ids = [transformer_image_retrieval_dataset.indexing_db_to_images_db[i] for i in relevant_image_indexing_ids]\n",
    "\t\t\trelevant_image_ids_as_int = [int(i) if isinstance(i, str) and i.isdigit() else i for i in relevant_image_db_ids]\n",
    "\t\t\t# Set the number of results to generate K\n",
    "\t\t\tresults_to_generate = 5\n",
    "\t\t\t# Get the top k image IDs for the first image in the retrieval dataset\n",
    "\t\t\ttop_k_image_ids = vision_transformer.generate_top_k_image_ids(test_image, results_to_generate, transformer_image_retrieval_dataset)\n",
    "\t\t\tpredicted_remapped_image_ids = [transformer_image_retrieval_dataset.indexing_db_to_images_db[int(i)] if int(i) in transformer_image_retrieval_dataset.indexing_db_to_images_db.keys() else str(i) for i in top_k_image_ids]\n",
    "\t\t\tpredicted_image_ids_as_int = [int(i) if isinstance(i, str) and i.isdigit() else i for i in predicted_remapped_image_ids]\n",
    "\t\t\t# Display the top k image IDs and the actual relevant image IDs\n",
    "\t\t\tprint(\"> Top 5 image IDs for image #\" + str(test_img_db_index) + \":\")\n",
    "\t\t\tprint(predicted_remapped_image_ids)\n",
    "\t\t\tprint(\"> Actual relevant image IDs (\" +  str(len(relevant_image_db_ids)) + \"):\")\n",
    "\t\t\tprint(relevant_image_db_ids)\n",
    "\t\t\t# Display the infos of relevant images\n",
    "\t\t\tprint(\"> Predicted related images:\")\n",
    "\t\t\tfor j in predicted_image_ids_as_int:\n",
    "\t\t\t\tif isinstance(j, int):\n",
    "\t\t\t\t\tprint(\"  - Image #\", j, \" (ImageNet ID: \", images_to_return[j][\"image_id\"], \") | Class: \\\"\", images_to_return[j]['image_label_name'],\"\\\"\", sep=\"\")\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(\"  - Image \\\"\", j, \"\\\" (original image index) | NOT in DB\", sep=\"\")\n",
    "\t\t\tprint(\"> Actual related images:\")\n",
    "\t\t\tfor j in relevant_image_ids_as_int:\n",
    "\t\t\t\tprint(\"  - Image #\", j, \" (ImageNet ID: \", images_to_return[j][\"image_id\"], \") | Class: \\\"\", images_to_return[j]['image_label_name'],\"\\\"\", sep=\"\")\n",
    "\t\t\t# Visualize the image to test and the top k images\n",
    "\t\t\ttest_image = utils.get_image_from_b64_string(images_to_return[test_img_db_index]['image_data'])\n",
    "\t\t\ttop_k_images = []\n",
    "\t\t\tfor j in predicted_remapped_image_ids:\n",
    "\t\t\t\t# Convert the index into an integer, if it is a number strung\n",
    "\t\t\t\tif isinstance(j, str):\n",
    "\t\t\t\t\tj = int(j) if j.isdigit() else j\n",
    "\t\t\t\tpredicted_img_data = images_to_return[j]['image_data']\n",
    "\t\t\t\tif isinstance(j, int) and predicted_img_data is not None:\n",
    "\t\t\t\t\ttop_k_images.append(utils.get_image_from_b64_string(predicted_img_data))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttop_k_images.append(j)\n",
    "\t\t\tnum_of_correct_predictions = len(set(predicted_image_ids_as_int).intersection(relevant_image_ids_as_int))\n",
    "\t\t\tprint(\"> Correct predicted image IDs: \", num_of_correct_predictions, \" / \", results_to_generate, sep=\"\")\n",
    "\t\t\t# Display the image to test and the top k images\n",
    "\t\t\tshowActualImages = True\n",
    "\t\t\tif showActualImages:\n",
    "\t\t\t\tfig, axs = plt.subplots(1, results_to_generate+1, figsize=(15, 5))\n",
    "\t\t\t\taxs[0].imshow(test_image)\n",
    "\t\t\t\taxs[0].axis('off')\n",
    "\t\t\t\taxs[0].set_title(\"Image to Test\")\n",
    "\t\t\t\tfor k in range(results_to_generate):\n",
    "\t\t\t\t\tif isinstance(top_k_images[k], np.ndarray):\n",
    "\t\t\t\t\t\taxs[k+1].imshow(top_k_images[k])\n",
    "\t\t\t\t\t\taxs[k+1].axis('off')\n",
    "\t\t\t\t\t\taxs[k+1].set_title(\"Predicted image #\" + str(k+1))\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\taxs[k+1].axis('off')\n",
    "\t\t\t\t\t\taxs[k+1].set_title(\"Image #\" + str(top_k_images[k]) + \"\\nNOT in DB\")\n",
    "\t\t\t\tplt.show()\n",
    "\t\t\t# Display all the actual relevant images from the original imagenet database, in a single row\n",
    "\t\t\tfig, axs = plt.subplots(1, len(relevant_image_ids_as_int), figsize=(15, 5))\n",
    "\t\t\tfor j in range(len(relevant_image_ids_as_int)):\n",
    "\t\t\t\timagenet_image_id = images_to_return[relevant_image_ids_as_int[j]][\"image_id\"]\n",
    "\t\t\t\tpil_image = imagenet_data['train'][imagenet_image_id]['image']\n",
    "\t\t\t\taxs[j].axis('off')\n",
    "\t\t\t\taxs[j].imshow(pil_image)\n",
    "\t\t\t\taxs[j].set_title(\"Related #\" + str(j+1))\n",
    "\t\t\tplt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
